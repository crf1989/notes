\documentclass{book}

\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}

\newcommand{\bra}[1]{{\langle#1|}}
\newcommand{\ket}[1]{{|#1\rangle}}
\newcommand{\inprod}[2]{\langle#1|#2\rangle}
\newcommand{\average}[1]{\langle#1\rangle}
\renewcommand{\Re}{{\rm Re}}
\renewcommand{\Im}{{\rm Im}}
\newcommand{\Tr}{{\rm Tr}}
\numberwithin{equation}{section}

\title{Notes}
\date{}
\begin{document}
%%\maketitle
\tableofcontents

\chapter{Useful formulas}
\section{Gaussian Integrals}
For a positive number $a$,
\begin{equation}
  \int_{-\infty}^\infty dx e^{-ax^2}=\sqrt{\frac{\pi}{a}},\quad
  \int\frac{dz^*dz}{2\pi i}e^{-z^*az}=\frac{1}{a}\quad.
\end{equation}

For real multi-dimensional integrals,
\begin{equation}
  \int\frac{dx_1\cdots dx_n}{(2\pi)^{\frac{n}{2}}}
  e^{-\frac{1}{2}\sum_{ij}x_iA_{ij}x_j+\sum_ix_iJ_i}=
  [\det A]^{-\frac{1}{2}}e^{\frac{1}{2}\sum_{ij}J_iA^{-1}_{ij}J_j}\quad.
\end{equation}

For complex multi-dimensional integrals,
\begin{equation}
  \int\left(\prod_{i=1}^n\frac{dz_i^*dz_i}{2\pi i}\right)
  e^{-\sum_{ij}z_i^*H_{ij}z_j+\sum_i(J^*_iz_i+z_i^*J_i)}=
  [\det H]^{-1}e^{\sum_{ij}J_i^*H_{ij}J_j}\quad.
\end{equation}

For Grassmann variables integrals,
\begin{equation}
  \int\left(\prod_{i=1}^n d\eta_i^*d\eta_i\right)
  e^{-\sum_{ij}\eta_i^*H_{ij}\eta_j+\sum_i(\xi_i^*\eta_i+\eta_i^*\xi_i)}
  =[\det H]e^{\sum_{ij}\xi_i^*H_{ij}\xi_j}\quad.
\end{equation}
%%------------------------------------------------------------------------

\section{Gaussian Distribution}
\subsection{Gaussian Distribution for One Variable}
The Gaussian distribution for one variable can be written as
\begin{equation}
  w(x)=Ae^{-\frac{1}{2}\beta x^2}.
\end{equation}
The normalization constant $A$ is given by the condition $\int
w(x)dx=1$, thus
\begin{equation}
  w(x)=\sqrt{\frac{\beta}{2\pi}}e^{-\frac{1}{2}\beta x^2}.
\end{equation}
The mean square fluctuation is
\begin{equation}
  \average{x^2}=\int_{-\infty}^\infty x^2w(x)dx=\frac{1}{\beta},
\end{equation}
thus we can write the Gaussian distribution in the form
\begin{equation}
  w(x)=\frac{1}{\sqrt{2\pi\average{x^2}}}\exp
  \left(-\frac{x^2}{2\average{x^2}}\right).
\end{equation}

\subsection{Gaussian Distribution for More Than One Variable}
The Gaussian distribution for more than one variable is
\begin{equation}
  w(x_1,\cdots,x_n)=Ae^{-\frac{1}{2}\beta_{ik}x_ix_k},
\end{equation}
where $\beta_{ik}=\beta_{ki}$ and normalization condition for $A$ is
\begin{equation}
  \int w\;dx_1\cdots dx_n=1.
\end{equation}

The linear transformation 
\begin{equation}
  x_i=a_{ik}x'_k
\end{equation}
of $x_1,\cdots,x_n$ converts the quadratic form $\beta_{ik}$ into a sum
of squares $x'_ix'_i$. In order that
\begin{equation}
  \beta_{ik}x_ix_k=x'_ix'_i=x'_ix'_k\delta_{ik}
\end{equation}
should be valid, the transformation coefficients must satisfy the
relations
\begin{equation}
  \beta_{ik}a_{il}a_{km}=\delta_{lm}.
\end{equation}
The determinant of the matrix on the left of this equation is the
product of the determinant $\beta=|\beta_{ik}|$ and two determinants
$a=|a_{ik}|$. The determinant $\delta_{ik}=1$. The above relation
therefore shows that
\begin{equation}
  \beta a^2=1.
\end{equation}

The Jacobian of the linear transformation from the variables $x_i$ to
$x'_i$ is the determinant $a$. After the transformation, therefore,
the normalization integral separates into a product of $n$ identical
integrals
\begin{equation}
  Aa\left[\int_{-\infty}^\infty\exp\left(-\frac{1}{2}x'^2\right)dx'\right]^n
  =\frac{A}{\sqrt{\beta}}(2\pi)^{\frac{n}{2}}=1.
\end{equation}
Thus we obtain finally the Gaussian distribution for more than one
variables in the form
\begin{equation}
  w=\frac{\sqrt{\beta}}{(2\pi)^{\frac{n}{2}}}\exp\left(
  -\frac{1}{2}\beta_{ik}x_ix_k\right).
\end{equation}

Now let $S=-\frac{1}{2}\beta_{ik}x_ix_k$ and define the quantities
\begin{equation}
  X_i=-\frac{\partial S}{\partial x_i}=\beta_{ik}x_k,
\end{equation}
which we refer to as conjugate\footnote{when apply to thermodynamics
  we call it thermodynamically conjugate.} to the $x_i$. From the definition 
the mean value $\bar{x}_i$ is
\begin{equation}
  \bar{x}_i=\frac{\beta}{(2\pi)^{\frac{n}{2}}}\int\cdots\int
  x_i\exp\left(-\frac{1}{2}\beta_{ik}(x_i-\bar{x}_i)
  (x_k-\bar{x}_k)\right)dx_1\cdots dx_n,
\end{equation}
differentiating this equation with respect to $\bar{x}_k$ and then
putting all $\bar{x}_i$ to zero, we have
\begin{equation}
  \average{x_iX_k}=\delta_{ik}.
\end{equation}
Since $X_k=\beta_{kl}x_l=x_l\beta_{lk}$, the above equation can be written as 
$\average{x_ix_l}\beta_{lk}=\delta_{ik}$, whence 
\begin{equation}
  \average{x_ix_k}=\beta^{-1}_{ik}.
\end{equation}
Similarly,
\begin{equation}
  \average{X_iX_k}=\beta_{il}\average{x_lX_k}=\beta_{il}\delta_{lk},
\end{equation}
i.e.
\begin{equation}
  \average{X_iX_k}=\beta_{ik}.
\end{equation}


\section{Delta Function}
The $\delta$ function satisfies the following properties.
\begin{equation}
  \int\delta(x-a) f(x)=f(a).
\end{equation}
And $\delta$ function is an even function:
\begin{equation}
  \delta(-x)=\delta(x).
\end{equation}
For a non-zero scalar $\alpha$
\begin{equation}
  \delta(\alpha x)=\frac{\delta(x)}{|\alpha|}
\end{equation}
The $\delta$ function can be expressed as
\begin{equation}
  \delta(x-\alpha)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{ip(x-\alpha)}dp=
  \frac{1}{2\pi}\int_{-\infty}^\infty e^{-ip(x-\alpha)}dp.
\end{equation}

\section{Euler Integral}
\subsection{Euler Integral of The First Kind: Beta Function}
Euler integral of the first kind: the Beta function:
\begin{equation}
  B(a,b)=\int_0^1x^{a-1}(1-x)^{b-1}dx.
\end{equation}
The Beta function has the following properties:
\begin{enumerate}[(i)]
\item Substitute $x$ with $x=1-t$ and it is easy to get
  \begin{equation}
    B(a,b)=B(b,a).
  \end{equation}

\item When $b>1$, integrate by parts (note that $x^a=x^{a-1}-x^{a-1}(1-x)$)
  \begin{equation}
    \begin{array}{rcl}
      B(a,b) &=& \displaystyle\int_0^1(1-x)^{b-1}d\frac{x^a}{a}\\\vbox to 20pt{}
      &=& \displaystyle\left.\frac{x^a(1-x)^{b-1}}{a}\right|_0^1
      +\frac{b-1}{a}\int_0^1x^a(1-x)^{b-2}dx\\\vbox to 20pt{}
      &=& \displaystyle\frac{b-1}{a}\int_0^1x^{a-1}(1-x)^{b-2}dx-
      \frac{b-1}{a}\int_0^1x^{a-1}(1-x)^{b-1}dx\\\vbox to 20pt{}
      &=&\displaystyle\frac{b-1}{a}B(a,b-1)-\frac{b-1}{a}B(a,b),
    \end{array}
  \end{equation}
  thus
  \begin{equation}
    B(a,b)=\frac{b-1}{a+b-1}B(a,b-1).
  \end{equation}
  For $a>1$, it is similar that
  \begin{equation}
    B(a,b)=\frac{a-1}{a+b-1}B(a-1,b).
  \end{equation}

  Let $n$ be a positive integer, 
  \begin{equation}
    B(n,a)=B(a,n)=\frac{1\cdot2\cdot3\cdots(n-1)}{a\cdot(a+1)\cdot(a+2)\cdots
      (a+n-1)}.
  \end{equation}
  Let $m,n$ be two positive integers,
  \begin{equation}
    B(m,n)=\frac{(n-1)!(m-1)!}{(m+n-1)!}.
  \end{equation}

\item Substitute $x$ with $x=\frac{y}{1+y}$, here $y$ is a new
  variable runs from $0$ to $\infty$, then
  \begin{equation}
    B(a,b)=\int_0^\infty\frac{y^{a-1}}{(1+y)^{a+b}}dy.
  \end{equation}

\item If $b=1-a$ and $0<a<1$ then
  \begin{equation}
    B(a,1-a)=\int_0^\infty\frac{y^{a-1}}{1+y}dy,
  \end{equation}
  this is also a Euler integral,
  \begin{equation}
    B(a,1-a)=\frac{\pi}{\sin a\pi}\quad(0<a<1),
  \end{equation}
  especially we have
  \begin{equation}
    B(\frac{1}{2},\frac{1}{2})=\pi.
  \end{equation}
\end{enumerate}
%%-----------------------------------------------------------------------------

\subsection{Euler Integral of The Second Kind: Gamma Function}
Euler integral of the second kind: the Gamma function is defined as
\begin{equation}
  \Gamma(a)=\int_0^\infty x^{a-1}e^{-x}dx.
\end{equation}

The Euler-Gauss formula:
\begin{equation}
  \Gamma(a)=\lim_{n\to\infty}n^a\frac{1\cdot2\cdot3\cdots(n-1)}
  {a\cdot(a+1)\cdot(a+2)\cdots(a+n-1)}.
\end{equation}

The Gamma Function has the following properties:
\begin{enumerate}[(i)]
\item For $a>0$, $\Gamma(a)$ is smooth.
\item Integrate by parts we shall get
  \begin{equation}
    \Gamma(a+1)=a\Gamma(a),
  \end{equation}
  repeat this formula
  \begin{equation}
    \Gamma(a+n)=(a+n-1)(a+n-1)\cdots(a+1)\Gamma(a).
  \end{equation}
  Let $n$ be a positive integer, then
  \begin{equation}
    \Gamma(n+1)=n!\quad.
  \end{equation}
\item If $a\to+0$ then
  \begin{equation}
    \Gamma(a)=\frac{\Gamma(a+1)}{a}\to+\infty.
  \end{equation}
  If $a>n+1$ the
  \begin{equation}
    \Gamma(a)>n!\quad.
  \end{equation}
\item Relation to Beta function:
  \begin{equation}
    B(a,b)=\frac{\Gamma(a)c\cdot\Gamma(b)}{\Gamma(a+b)}.
  \end{equation}
\item if $0<a<1$ then
  \begin{equation}
    \Gamma(a)\Gamma(1-a)=\frac{\pi}{\sin a\pi},
  \end{equation}
  and 
  \begin{equation}
    \Gamma(\frac{1}{2})=\sqrt{\pi}.
  \end{equation}
\item 
  \begin{equation}
    \prod_{\nu=1}^{n-1}\Gamma(\frac{\nu}{n})=
    \frac{(2\pi)^{\frac{n-1}{2}}}{\sqrt{n}}.
  \end{equation}
\item Raabe's formula:
  \begin{equation}
    \int_a^{a+1}\ln\Gamma(t)dt=\frac{1}{2}\ln2\pi+a\ln a-a,\quad a>0,
  \end{equation}
  in particular, if $a=0$ then
  \begin{equation}
    \int_0^1\ln\Gamma(t)dt=\frac{1}{2}\ln2\pi.
  \end{equation}
\item Legendre formula:
  \begin{equation}
    \Gamma(a)\Gamma(a+\frac{1}{2})=\frac{\sqrt{\pi}}{2^{2a-1}}\Gamma(2a).
  \end{equation}
\end{enumerate}
%%---------------------------------------------------------------------------

\section{Baker-Campbell-Hausdorff Formula}
Baker-Campbell-Hausdorff Formula is
\begin{equation}
  e^ABe^{-A}=\sum_{n=0}^\infty\frac{1}{n!}[A,B]_n=
  B+[A,B]+\frac{1}{2}[A,[A,B]]+\frac{1}{6}[A,[A,[A,B]]]+\cdots\quad,
\end{equation}
this formula can be proved by defining $B(\tau)=e^{\tau A}Be^{-\tau A}$
and formally integrating its equation of motion $dB/d\tau=[A,B(\tau)]$.
%%------------------------------------------------------------------------

\section{Feynman Result}
The Feynman result reads
\begin{equation}
  e^{A+B}=e^Ae^Be^{-\frac{1}{2}[A,B]},
\end{equation}
which is true only if $[A,B]$ commutes with both $A$ and $B$.

To prove it, recall that
\begin{equation}
e^{\tau(A+B)}=e^{\tau A}T_\tau\exp\left[
  \int_0^\tau d\tau'e^{-\tau'A}Be^{\tau'A}\right]
\end{equation}
and evaluate the integral for $\tau=1$.
%%---------------------------------------------------------------------------

\section{Kubo Identity}
The Kubo Identity states that
\begin{equation}
  [e^{-\beta H},A]=e^{-\beta H}\int_0^\beta 
  e^{\lambda H}[A,H]e^{-\lambda H}d\lambda.
\end{equation}
To derive this relation, let us consider a quantity 
\begin{equation}
  S=e^{\lambda H}[A,e^{-\lambda H}]=e^{\lambda H}Ae^{-\lambda H}-A,
\end{equation}
differentiating it with respect to $\lambda$ yields
\begin{equation}
  \frac{dS}{d\lambda}=e^{\lambda H}[H,A]e^{-\lambda}.
\end{equation}
Therefore
\begin{equation}
  S(\beta)=S(0)+\int_0^\beta\frac{dS}{d\lambda}d\lambda=
  \int_0^\beta e^{\lambda H}[H,A]e^{-\lambda H}d\lambda,
\end{equation}
and accordingly
\begin{equation}
  [e^{-\beta H},A]=-e^{-\beta H}S(\beta)=e^{-\beta H}\int_0^\beta 
  e^{\lambda H}[A,H]e^{-\lambda H}d\lambda.
\end{equation}


\section{Laguerre Polynomials}
The Laguerre polynomials are solution of Laguerre's equation:
\begin{equation}
  xy''+(1-x)y'+ny=0,
\end{equation}
where $n$ is non-negative integer. The Laguerre polynomials is
\begin{equation}
  L_n(x)=\frac{e^x}{n!}\frac{d^n}{dx^n}(e^{-x}x^n)=
  \sum_{k=0}^n\frac{(-x)^k}{k!}\frac{n!}{k!(n-k)!}.
\end{equation}
The generating function is
\begin{equation}
  \frac{e^{-xt/(1-t)}}{1-t}=\sum_{n=0}^\infty L_n(x)t^n\quad.
\end{equation}
%%-------------------------------------------------------------------------

\section{Cramer's Rule}
Consider a system of $n$ linear equations of $n$ unknowns, represented in 
matrix multiplication form:
\begin{equation}
  Ax=b,
\end{equation}
where the $n\times n$ matrix $A$ has a nonzero determinant, and the
vector $x=(x_1,\cdots,x_n)^T$ is the column vector of the variables.
Then Cramer's rule states that the system has a unique solution, whose
individual values are given by:
\begin{equation}
  x_i=\frac{\det A_i}{\det A},
\end{equation}
where $A_i$ is the matrix formed by replacing the $i$-th column of $A$
by the column vector $b$.

%%-------------------------------------------------------------------------


\section{Sherman-Morrison Formula}
Suppose $A$ is an invertible square matrix and $u,v$ are column
vectors. Suppose that $1+v^TA^{-1}u\ne0$, then the Sherman-Morrison formula
states that
\begin{equation}
  (A+uv^T)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}.
\end{equation}
Here $uv^T$ is the outer product of two vectors $u$ and $v$.
\section{Simple Impurity Model at Zero Temperature}
The Hamiltonian of simple impurity model is defined as
\begin{equation}
  H=\sum_k\varepsilon_kc_k^\dag c_k+\sum_kV_k(c_k^\dag d+d^\dag c_k)
  +\varepsilon_0d^\dag d,
\end{equation}
let $H=H_0+V$, where
\begin{equation}
  H_0=\sum_k\varepsilon_kc_k^\dag c_k+\varepsilon_0d^\dag d,\quad
  V=\sum_kV_k(c_k^\dag d+d^\dag c_k).
\end{equation}

The Green's function is
\begin{equation}
  G(t)=-i\bra{0}Td(t)d^\dag\ket{0}=-i\bra{0}d(t)d^\dag\ket{0},
\end{equation}
apply Fourier transform on it, then
\begin{equation}
  G(\omega)=\bra{0}d\frac{1}{\omega+i0-H}d^\dag\ket{0}.
\end{equation}

Notice that
\begin{equation}
  \begin{array}{rcl}
  \displaystyle\frac{1}{\omega-H}&=&\displaystyle\frac{1}{\omega-H_0}+
  \frac{1}{\omega-H_0}V\frac{1}{\omega-H}\\\vbox to 20pt{}
  &=&\displaystyle\frac{1}{\omega-H_0}+
  \frac{1}{\omega-H_0}V\frac{1}{\omega-H_0}+
  \frac{1}{\omega-H_0}V\frac{1}{\omega-H_0}V\frac{1}{\omega-H},
  \end{array}
\end{equation}
the second term produce just 0, thus 
\begin{equation}
  \begin{array}{rcl}
  G(\omega)&=&\displaystyle\bra{0}d\frac{1}{\omega-H_0}d^\dag\ket{0}+
  \bra{0}d\frac{1}{\omega-H_0}V\frac{1}{\omega-H_0}V
  \frac{1}{\omega-H}d^\dag\ket{0}\\\vbox to 20pt{}
  &=&\displaystyle\frac{1}{\omega-\varepsilon_0}+
  \frac{1}{\omega-\varepsilon_0}\bra{0}dV\frac{1}{\omega-H_0}V
  \frac{1}{\omega-H}d^\dag\ket{0}\\\vbox to 20pt{}
  &=&\displaystyle\frac{1}{\omega-\varepsilon_0}+
  \frac{1}{\omega-\varepsilon_0}
  \bra{0}d\sum_kd^\dag c_k\frac{V_k^2}{\omega-H_0}c_k^\dag d
  \frac{1}{\omega-H}d^\dag\ket{0}\\
  &=&\displaystyle\frac{1}{\omega-\varepsilon_0}+
  \frac{1}{\omega-\varepsilon_0}
  \sum_k\frac{V_k^2}{\omega-\varepsilon_k}G(\omega).
  \end{array}
\end{equation}
Therefore 
\begin{equation}
  G^{-1}(\omega)=\omega-\varepsilon_0-
  \sum_k\frac{V_k^2}{\omega-\varepsilon_k},
\end{equation}
it can be written as
\begin{equation}
  G^{-1}(\omega)=\omega-\varepsilon_0-\int_{-\infty}^\infty d\varepsilon
  \frac{\Delta(\varepsilon)}{\omega-\varepsilon},
\end{equation}
where
\begin{equation}
  \Delta(\varepsilon)=\sum_kV_k^2\delta(\varepsilon-\varepsilon_k).
\end{equation}

Now consider $V$ is in site representation:
\begin{equation}
  V=\sum_{i}(t_{io}c_i^\dag d+t_{oi}d^\dag c_i),
\end{equation}
then we have that
\begin{equation}
  \begin{array}{rcl}
    G(\omega) &=& \displaystyle\frac{1}{\omega-\varepsilon_0}
    +\frac{1}{\omega-\varepsilon_0}\sum_{ij}t_{oi}t_{jo}\bra{0}dd^\dag c_i
    \frac{1}{\omega-H_0}c_j^\dag
    d\frac{1}{\omega-H}d^\dag\ket{0}\\\vbox to 20pt{}
    &=&\displaystyle\frac{1}{\omega-\varepsilon_0}
    +\frac{1}{\omega-\varepsilon_0}\sum_{ij}t_{oi}t_{jo}
    G_{ij}^{(o)}(\omega)G(\omega),\\
  \end{array}
\end{equation}
thus
\begin{equation}
  G^{-1}(\omega)=\omega-\varepsilon_0-\sum_{ij}t_{oi}t_{jo}G^{(o)}_{ij}(\omega),
\end{equation}
where $G^{(o)}_{ij}$ is the Green's function with one site removed.
%%----------------------------------------------------------------------------

\section{Green's Function for Simple Cubic Lattice}
The first Brillouin zone for the simple cubic lattice is the cube
\begin{equation}
  -\pi/a\le k_x<\pi/a,\quad-\pi/a\le k_x<\pi/a, \quad-\pi/a\le k_x<\pi/a,
\end{equation}
where $a$ is the lattice constant. The diagonal matrix element of
Green's function is
\begin{equation}
  G(\omega)=\frac{a^3}{(2\pi)^3}\int_{-\pi/a}^{\pi/a}dk_x
  \int_{-\pi/a}^{\pi/a}dk_y\int_{-\pi/a}^{\pi/a}dk_z
  \frac{1}{\omega-2t(\cos k_xa+\cos k_ya+\cos k_za)},
\end{equation}
introducing the variable $x=k_xa,y=k_ya,z=k_za$ we obtain
\begin{equation}
  G(\omega)=\frac{1}{(2\pi)^3}\int_{-\pi}^\pi dx\int_{-\pi}^\pi dy
  \int_{-\pi}^\pi dz
  \frac{1}{\omega-2t(\cos x+\cos y+\cos z)}.
  \label{simple-cubic-green-function}
\end{equation}

This function can be expressed by complete elliptic integral. The
complete elliptic integral of the first kind $K(k)$ as complex
function of the complex modulus $k$ is defined by
\begin{equation}
  K(k)=\int_0^{\frac{\pi}{2}}d\theta(1-k^2\sin^2\theta)^{-\frac{1}{2}},
\end{equation}
this function is an even function and $K(k^*)=K(k)^*$.

After the integration over $y$ and $z$, the integral
\eqref{simple-cubic-green-function} yields 
\begin{equation}
  G(\omega)=\frac{1}{2\pi^2t}\int_0^\pi kK(k)dx,
\end{equation}
where
\begin{equation}
  k=\frac{4t}{\omega-2t\cos x}.
\end{equation}

For simple cubic lattice, $\Re G$ is an odd function of $\omega$ and
$\Im G$ is an even function:
\begin{equation}
  \Re\;G(\omega)=-\Re\;G(\omega),\quad \Im\;G(\omega)=\Im\;G(\omega),
\end{equation}
hence we have only to consider the range $0\le\omega\le6t$ in the
following. The Green's function can calculated numerically, when $0<\omega<2t$,
\begin{equation}
  \begin{array}{l}
    \displaystyle\Re\;G(\omega)=-\frac{1}{2\pi^2t}\int_0^{\cos^{-1}(\omega/2t)}
    dxK\left(\frac{1}{|k|}\right)+\frac{1}{2\pi^2t}
    \int_{\cos^{-1}(\omega/2t)}^{\pi}K\left(\frac{1}{k}\right),\\\vbox
    to 20pt{} \displaystyle\Im\;G(\omega)=\frac{1}{\pi^2}\int_0^\pi
    dxK \left(\frac{\sqrt{k^2-1}}{k}\right),\\
  \end{array}
\end{equation}
when $2t\le\omega<6t$,
\begin{equation}
  \begin{array}{l}
    \displaystyle\Re\;G(\omega)=\frac{1}{2\pi^2t}
    \int_0^{\cos^{-1}[(\omega-4t)/2t]}dxK\left(\frac{1}{k}\right)
    +\frac{1}{2\pi^2t}\int_{\cos^{-1}[(\omega-4t)/2t]}^\pi dxK(k),\\\vbox to 20pt{}
    \displaystyle\Im\;G(\omega)=\frac{1}{2\pi^2t}
    \int_0^{\cos^{-1}[(\omega-4t)/2t]}dx K\left(\frac{\sqrt{k^2-1}}{k}\right).\\
  \end{array}
\end{equation}

\chapter{Coherent States}
Coherent states is defined as the eigenstates of annihilation operator:
\begin{equation}
  a_\alpha\ket{\phi}=\phi_\alpha\ket{\phi}.
\end{equation}

\section{Boson Coherent States}
Boson coherent states:
\begin{equation}
  \ket{\phi}=e^{\sum_\alpha\phi_\alpha a_\alpha^\dag}\ket{0},\quad
  \bra{\phi}=\bra{0}e^{\sum_\alpha\phi_\alpha^* a_\alpha}\quad,
\end{equation}
where $\phi_\alpha$ is complex number.

The overlap of two coherent states:
\begin{equation}
  \inprod{\phi}{\phi'}=e^{\sum_\alpha\phi_\alpha^*\phi'_\alpha}\quad.
\end{equation}

The overcompleteness in the Fock space:
\begin{equation}
  \int\left(\prod_\alpha\frac{d\phi_\alpha^*d\phi_\alpha}{2\pi i}\right)
  e^{-\sum\phi_\alpha^*\phi_\alpha}\ket{\phi}\bra{\phi}=1,
\end{equation}
where
\begin{equation}
  \frac{d\phi_\alpha^*d\phi_\alpha}{2\pi i}=
  \frac{d({\rm Re}\phi_\alpha)d({\rm Im}\phi_\alpha)}{\pi}\quad.
\end{equation}

The trace of an operator $A$ in Fock space can be written as
\begin{equation}
  {\rm Tr}A=
  \int\left(\prod_\alpha\frac{d\phi_\alpha^*d\phi_\alpha}{2\pi i}\right)
  e^{-\sum\phi_\alpha^*\phi_\alpha}\bra{\phi}A\ket{\phi}\quad.
\end{equation}

The average particle number of a coherent state is
\begin{equation}
  \bar{N}=\frac{\bra{\phi}N\ket{\phi}}{\inprod{\phi}{\phi}}
  =\frac{\bra{\phi}\sum_\alpha a_\alpha^\dag a_\alpha\ket{\phi}}{\inprod{\phi}{\phi}}
  =\sum_\alpha\phi_\alpha^*\phi_\alpha,
\end{equation}
and the variance is
\begin{equation}
  \sigma^2=\frac{\bra{\phi}N^2\ket{\phi}}{\inprod{\phi}{\phi}}-\bar{N}^2
  =\bar{N}\quad.
\end{equation}
%%-----------------------------------------------------------------------------

\section{Grassmann Algebra}
The Grassmann numbers is defined to be anticommuting numbers:
\begin{equation}
  \xi_\alpha\xi_\beta+\xi_\beta\xi_\alpha=0,\quad \xi_\alpha^2=0\quad.
\end{equation}

The conjugation of a Grassmann number is defined as
\begin{equation}
  (\xi_\alpha)^*=\xi_\alpha^*,\quad(\xi_\alpha^*)^*=\xi_\alpha\quad.
\end{equation}

If $\lambda$ is a complex number,
\begin{equation}
  (\lambda\xi_\alpha)^*=\lambda^*\xi_\alpha,
  \label{anticommuting}
\end{equation}
and for any product of Grassmann numbers:
\begin{equation}
  (\xi_1\cdots\xi_n)^*=\xi_n^*\xi_{n-1}^*\cdots\xi_1^*\quad,
\end{equation}
and for combinations of Grassmann variables and creation and
annihilation operators
\begin{equation}
  \xi a+a\xi=0,\quad(\xi a)^\dag=a^\dag\xi^*\quad.
\end{equation}

Because of property \eqref{anticommuting}, 
\begin{equation}
  f(\xi)=f_0+f_1\xi,\quad 
  A(\xi^*,\xi)=a_0+a_1\xi+\bar{a}_1\xi^*+a_{12}\xi^*\xi,
\end{equation}
in particular,
\begin{equation}
  e^{-\lambda\xi}=1-\lambda\xi\quad.
\end{equation}

A derivative can be defined for Grassmann variable function,
\begin{equation}
  \frac{\partial}{\partial\xi}(\xi^*\xi)=
  \frac{\partial}{\partial\xi}(-\xi\xi^*)=-\xi^*\quad.
\end{equation}

And a integral can be defined as
\begin{equation}
  \int d\xi\,1=0,\quad\int d\xi\,\xi=1,
  \quad\int d\xi^*\,1=0,\quad\int d\xi^*\,\xi^*=1,
\end{equation}
to remember,
\begin{equation}
  \int d\xi=\frac{\partial}{\partial\xi},\quad
  \int d\xi^*=\frac{\partial}{\partial\xi^*}\quad.
\end{equation}
%%--------------------------------------------------------------------------


\section{Fermion Coherent States}
Fermion Coherent States is defined as
\begin{equation}
  \ket{\xi}=e^{-\sum_\alpha\xi_\alpha a_\alpha^\dag}\ket{0}
  =\prod_\alpha(1-\xi_\alpha a_\alpha^\dag)\ket{0},
\end{equation}
we can verify that $a_\alpha\ket{\xi}=\xi_\alpha\ket{\xi}$ by using
\begin{equation}
  \xi_\alpha\ket{0}=\xi_\alpha(1-\xi_\alpha a^\dag)\ket{0}\quad.
\end{equation}

Similarly, the adjoint of the coherent states is
\begin{equation}
  \bra{\xi}=\bra{0}e^{-\sum_\alpha a_\alpha\xi_\alpha^*}
  =\bra{0}e^{\sum_\alpha\xi_\alpha^*a_\alpha}\quad.
\end{equation}

The overlap of two coherent states is 
\begin{equation}
  \inprod{\xi}{\xi'}=\prod_\alpha(1+\xi_\alpha^*\xi'_\alpha)
  =e^{\sum_\alpha\xi_\alpha^*\xi'_\alpha}\quad.
\end{equation}

The closure relation can be written as
\begin{equation}
  \int\left(\prod_\alpha d\xi_\alpha^*d\xi_\alpha\right)
  e^{-\sum_\alpha\xi_\alpha^*\xi_\alpha}\ket{\xi}\bra{\xi}=1\quad.
\end{equation}

The trace of an operator $A$ in Fock space can be written as
\begin{equation}
  {\rm Tr}A=\int\left(\prod_\alpha d\xi_\alpha^*d\xi_\alpha\right)
  e^{-\sum_\alpha\xi_\alpha^*\xi_\alpha}\bra{-\xi}A\ket{\xi},
\end{equation}
note the anti periodic condition here.
%%------------------------------------------------------------------------------


\chapter{Linear Response}
\section{Perturbations Depending on Time}
We now seek the solution of the perturbed equation
\begin{equation}
  i\hbar\frac{\partial\Psi(t)}{\partial t}=[H_0+V(t)]\Psi(t),
\end{equation}
in the form of a sum
\begin{equation}
  \Psi(t)=\sum_ka_k(t)\psi_k(t),
\end{equation}
where the expansion coefficients $a_k(t)$ are functions of time,
and $\psi_k(t)$ are unperturbed stationary wave functions:
\begin{equation}
  i\hbar\frac{\partial\psi_k(t)}{\partial t}=H_0\psi_k(t)=E_k^{(0)}\psi_k(t).
\end{equation}
Therefore we obtain that
\begin{equation}
  i\hbar\sum_k\psi_k(t)\frac{da_k(t)}{dt}=\sum_ka_k(t)V(t)\psi_k(t),
\end{equation}
multiplying both sides of this equation on the left by $\psi_m(t)$ and
integrating then
\begin{equation}
  i\hbar\frac{da_m(t)}{dt}=\sum_kV_{mk}(t)a_k(t),
\end{equation}
where
\begin{equation}
  V_{mk}(t)=\bra{m}V\ket{k}e^{i\omega_{mk}t}=V_{mk}e^{i\omega_{mk}t},\quad
  \omega_{mk}=\frac{E_{m}^{(0)}-E_k^{(0)}}{\hbar}.
\end{equation}

Let the unperturbed wave function be $\psi_n(t)$, i.e. $a_n^{(0)}=1$
and $a_k^{(0)}=0$ for $k\ne n$. To find the first approximation, we
seek $a_k=a_k^{0}+a_k^{(1)}$, substituting $a_k=a_k(0)$ we find
\begin{equation}
  i\hbar \frac{da_k^{(1)}(t)}{dt}=V_{kn}(t),
\end{equation}
integrating it gives
\begin{equation}
  a_{kn}^{(0)}(t)=-\frac{i}{\hbar}\int V_{kn}e^{i\omega_{kn}t}dt.
\end{equation}
%%----------------------------------------------------------------------------

\section{Fermi Golden Rule}
Let the perturbation be
\begin{equation}
  V(t)=Ve^{-i\omega t},
\end{equation}
then
\begin{equation}
  a_{fi}=-\frac{i}{\hbar}\int_0^tV_{fi}(t)dt=
  -V_{fi}\frac{e^{i(\omega_{fi}-\omega)t}-1}{\hbar(\omega_{fi}-\omega)}.
\end{equation}
Therefore the squared modulus of $a_{fi}$ is
\begin{equation}
  |a_{fi}|^2=|V_{fi}|^2\frac{4\sin^2[\frac{1}{2}(\omega_{fi}-\omega)t]}
{\hbar^2(\omega_{fi}-\omega)^2},
\end{equation}
noticing that $\lim_{t\to\infty}\frac{\sin^a\alpha t}{\pi t\alpha^2}=\delta(\alpha)$ we have
\begin{equation}
  |a_{fi}|^2=\frac{2\pi}{\hbar}|V_{fi}|^2\delta(E_f-E_i-\hbar\omega)t.
\end{equation}
Thus the probability $dw_{fi}$ of the transition rate per unit time is
\begin{equation}
  dw_{fi}=\frac{2\pi}{\hbar}|V_{fi}|^2\delta(E_f-E_i-\hbar\omega).
\end{equation}

Another method to derive the above formula is that let 
\begin{equation}
  V(t)=Ve^{-i\omega t+\eta t},
\end{equation}
and integrating from $t=-\infty$ to $t=0$, then
\begin{equation}
  |a_{fi}|^2=\frac{1}{\hbar^2}|V_{fi}|^2\frac{e^{2\eta t}}
{(\omega_{fi}-\omega)^2+\eta^2}
\end{equation}
Then the transition rate is [note that
$\lim_{\eta\to0}\frac{\eta}{\pi(\alpha^2+\eta^2)}=\delta(\alpha)$]
\begin{equation}
  \frac{d}{dt}|a_{fi}|^2=\frac{2\pi}{\hbar}|V_{fi}|^2\delta(E_f-E_i-\hbar\omega).
\end{equation}
%%----------------------------------------------------------------------------


\section{The Generalized Susceptibility}
When there exists an external interaction, the perturbing operator can
be written as
\begin{equation}
  V=-xf(t),  
\end{equation}
where $x$ is the operator of the physical quantity concerned, and the
perturbing generalized force $f$ is a given function of time.

The quantum mean value $\bar{x}(t)$ is given by a formula of the type
\begin{equation}
  \bar{x}(t)=\int_0^\infty\alpha(\tau)f(t-\tau)d\tau,
\end{equation}
where $\alpha(\tau)$ being a function of time which depends on the
properties of the body.

Applying fourier transform on both sides of this formula
\begin{equation}
  \int_0^\infty\bar{x}(t)e^{i\omega t}dt=
  \int_0^\infty\alpha(\tau)f(t-\tau)e^{i\omega t}d\tau dt,
\end{equation}
we obtain that
\begin{equation}
  \bar{x}(\omega)=\alpha(\omega)f(\omega).
\end{equation}

If the function $f$ is purely monochromatic and is given by the real
expression
\begin{equation}
  f(t)=\frac{1}{2}(f_0e^{-i\omega t}+f_0^*e^{i\omega t}),
  \label{generalized-force}
\end{equation}
then we shall have 
\begin{equation}
  \bar{x}(t)=\frac{1}{2}[\alpha(\omega)f_0e^{-i\omega t}
    +\alpha(-\omega)f_0^*e^{i\omega t}]
  \label{generalized-x}
\end{equation}

The function $\alpha(\omega)$ has the similar properties as retarded
Green's function:
\begin{equation}
  \alpha(-\omega)=\alpha^*(\omega),
\end{equation}
i.e.,
\begin{equation}
  \Re\,\alpha(-\omega)=\Re\,\alpha(\omega),\quad
  \Im\,\alpha(-\omega)=-\Im\,\alpha(\omega).
\end{equation}
And the Kramers-Kronig relations:
\begin{equation}
  \Re\,\alpha(\omega)=-\frac{1}{\pi}P\int_{-\infty}^\infty
  \frac{\Im\,\alpha(\varepsilon)}{\omega-\varepsilon}d\varepsilon,\quad
  \Im\,\alpha(\omega)=\frac{1}{\pi}P\int_{-\infty}^\infty
  \frac{\Re\,\alpha(\varepsilon)}{\omega-\varepsilon}d\varepsilon.
\end{equation}

The energy change per unit time of the system is just
$dE/dt=\overline{\partial H/\partial t}$, since only the perturbation
$V$ in Hamiltonian depends on explicitly on time, we have
\begin{equation}
  \frac{dE}{dt}=-\bar{x}\frac{df}{dt}.
\end{equation}
Substituting $\bar{x}$ and $f$ from \eqref{generalized-force} and
\eqref{generalized-x} and averaging over time, the terms containing 
$e^{2i\omega t}$ vanish, and we obtain 
\begin{equation}
  Q=\frac{1}{4}i\omega(\alpha^*-\alpha)|f_0|^2=
  \frac{1}{2}\omega\Im\,\alpha(\omega)|f_0|^2,
\end{equation}
where $Q$ is the mean energy dissipated per unit time.

\section{The Fluctuation Dissipation Theorem}
Let us now assume that the system is at state $\ket{n}$ and is subject to a
periodic perturbation, described by the operator
\begin{equation}
  V=-xf=-\frac{1}{2}x(f_0e^{-i\omega t}+f_0^*e^{i\omega t}).
\end{equation}
Using Fermi Golden Rule, the transition rate from state $n$ to state
$m$ per unit time is given by
\begin{equation}
  w_{mn}=\frac{\pi|f_0|^2}{2\hbar^2}|x_{mn}|^2
  [\delta(\omega+\omega_{mn})+\delta(\omega+\omega_{nm})].
\end{equation}
The dissipation per unit time is given by
\begin{equation}
  Q=\sum_{m}w_{mn}\hbar\omega_{mn}=\frac{\pi}{2\hbar}|f_0|^2
  \sum_m|x_{mn}|^2[\delta(\omega+\omega_{mn})+\delta(\omega+\omega_{nm})]
  \omega_{mn},
\end{equation}
or, since the delta function zero except when their argument is zero,
\begin{equation}
  Q=\frac{\pi}{2\hbar}\omega|f_0|^2
  \sum_m|x_{mn}|^2[\delta(\omega+\omega_{nm})-\delta(\omega+\omega_{mn})],
\end{equation}
thus
\begin{equation}
  \Im\,\alpha(\omega)=\frac{\pi}{\hbar}\sum_m|x_{mn}|^2
  [\delta(\omega+\omega_{nm})-\delta(\omega+\omega_{mn})].
\end{equation}

Now define 
\begin{equation}
  (x^2)_\omega=\int_{-\infty}^\infty\frac{1}{2}
  \average{x(t)x(0)+x(0)x(t)}e^{i\omega t}dt,
\end{equation}
in canonical ensemble it is
\begin{equation}
  (x^2)_\omega=\pi\sum_{nm}\rho_n|x_{mn}|^2
  [\delta(\omega+\omega_{nm})+\delta(\omega+\omega_{mn})],
\end{equation}
where $\rho_n=e^{(F-E_n)/T}$, $E_n$ denotes the energy levels and $F$
is free energy. Since the summation is now over both $m$ and $n$,
these can be interchanged:
\begin{equation}
  \begin{array}{rcl}
    (x^2)_\omega&=&\displaystyle\pi\sum_{mn}(\rho_n+\rho_m)|x_{mn}|^2
    \delta(\omega+\omega_{nm})\\\vbox to 18pt{}
    &=&\displaystyle\pi\sum_{mn}\rho_n(1+e^{-\hbar\omega_{mn}/T})|x_{mn}|^2
    \delta(\omega+\omega_{nm})\\\vbox to 18pt{}
    &=&\displaystyle\pi(1+e^{-\hbar\omega/T})\sum_{mn}\rho_n|x_{mn}|^2
    \delta(\omega+\omega_{nm}).
  \end{array}
\end{equation}
Similarly, in canonical ensemble
\begin{equation}
  \Im\,\alpha(\omega)=\frac{\pi}{\hbar}(1-e^{-\hbar\omega/T})
  \sum_{mn}\rho_n|x_{nm}|^2\delta(\omega+\omega_{nm}),
\end{equation}
a comparison of these two expressions gives
\begin{equation}
  (x^2)_\omega=\hbar\Im\,\alpha(\omega)\coth\frac{\hbar\omega}{2T}.
\end{equation}
The mean square of the fluctuating quantity is given by the
integration
\begin{equation}
  \average{x^2}=\frac{\hbar}{\pi}\int_0^\infty\Im\,\alpha(\omega)
  \coth\frac{\hbar\omega}{2T}d\omega.
\end{equation}

\section{Kubo Greenwood Formula}
Now write the perturbing operator as
\begin{equation}
  V=-\int\vec{j}\cdot\vec{A}dx,
\end{equation}
let $\alpha(\omega)$ denotes the corresponding generalized
susceptibility then the mean energy dissipated per unit time and per
unit volume is
\begin{equation}
  Q=\frac{1}{2}\omega\Im\,\alpha(\omega)|\vec{A}|^2.
\end{equation}
However, this generalized susceptibility is not the conductivity, to
get the conductivity, recall that
\begin{equation}
  \vec{E}(t)=-\frac{\partial\vec{A}}{\partial t},
\end{equation}
therefore
\begin{equation}
  \vec{E}(\omega)=i\omega\vec{A},
\end{equation}
which means
\begin{equation}
  j(\omega)=\alpha(\omega)A(\omega)=\frac{\alpha(\omega)}{i\omega}E(\omega),
\end{equation}
or
\begin{equation}
  \sigma(\omega)=\frac{\alpha(\omega)}{i\omega}.
\end{equation}

Thus the dissipated term written in conductivity is just
\begin{equation}
  Q=\frac{1}{2}\omega\Im\,\alpha(\omega)|A|^2=\frac{1}{2}\Re\,\sigma(\omega)\;|E|^2,
\end{equation}
and
\begin{equation}
  \Re\,\sigma=\frac{\Im\,\alpha}{\omega}=
  \frac{\pi}{\hbar\omega}\sum_{mn}(\rho_n-\rho_m)
  |j_{mn}|^2\delta(\omega+\omega_{nm}).
\end{equation}

Now there comes an assumption which is called ``independent particle
approximation'': we replace $\rho$ by single electron distribution
function $f$ and recall that $j=-ev$ then
\begin{equation}
  \Re\,\sigma=\frac{\hbar\pi e^2}{V}\sum_{mn}
  \frac{f_n-f_m}{\hbar\omega_{mn}}|v_{mn}|^2\delta(E_n+\hbar\omega-E_m),
\end{equation}
where $V$ is the volume which acts as normalized factor.
Notice that 
\begin{equation}
  \frac{f_n-f_m}{\hbar\omega_{mn}}\delta(E_n+\hbar\omega-E_m)=
  \int dE\frac{f(E)-f(E+\hbar\omega)}{\hbar\omega}
  \delta(E-E_n)\delta(E_n+\hbar\omega-E_m),
\end{equation}
then the formula of $\Re\,\sigma$ become (recall that
$\frac{1}{\pi}\Im G$ is the density of states)
\begin{equation}
  \begin{array}{rcl}
    \Re\,\sigma(\omega)&=&\displaystyle\frac{\hbar\pi e^2}{V} \int
    dE\frac{f(E)-f(E+\hbar\omega)}{\hbar\omega}\sum_{nm}
    v_{nm}\delta(E_n+\hbar\omega-E_m)v_{mn}\delta(E-E_n)\\\vbox to
    20pt{} &=&\displaystyle\frac{\hbar e^2}{\pi V}\int dE
    \frac{f(E)-f(E+\hbar\omega)}{\hbar\omega}
    \Tr[v\Im\,G^R(E+\hbar\omega)v\Im\,G^R(E)].
  \end{array}
\end{equation}
For static conductivity, we have 
\begin{equation}
  \lim_{\omega\to0}\Re\,\sigma(\omega)=
  \frac{e^2\hbar}{\pi V}\int dE
  \left(-\frac{\partial f}{\partial E}\right)
  \sum_k|\bra{k}v\ket{k}|^2|\Im\,G^R(E,k)|^2,
\end{equation}
or in three dimension
\begin{equation}
  \lim_{\omega\to0}\Re\,\sigma(\omega)=
  \frac{e^2\hbar}{\pi}\int dE
  \left(-\frac{\partial f}{\partial E}\right)
  \int \frac{d^3k}{(2\pi)^3}\; v^2(k)|\Im\,G^R(E,k)|^2.
\end{equation}

\section{Green Kubo Formula}
Let $\Psi^{(0)}_n$ be the wave function of the unperturbed system,
then applying equations of perturbations depending on time in first
approximation we have
\begin{equation}
  \Psi_n=\Psi^{(0)}_n+\sum_ma_{m}\Psi^{(0)}_m,
\end{equation}
where $a_m$ satisfy the equation
\begin{equation}
  i\hbar\frac{da_m}{dt}=V_{mn}e^{i\omega_{mn}t}=-\frac{1}{2}
  x_{mn}e^{i\omega_{mn}t}(f_0e^{-i\omega t}+f_0^*e^{i\omega t}).
\end{equation}
In solving this, we must assume that the perturbation is ``adiabatically''
applied until the time $t$ from $t=-\infty$, this means that we must put
$\omega\to\omega\mp i0$ in factors $e^{\pm i\omega t}$. Then
\begin{equation}
  a_m=\frac{1}{2\hbar}x_{mn}e^{i\omega_{mn}t}\left[
    \frac{f_0e^{-i\omega t}}{\omega_{mn}-\omega-i0}+
    \frac{f_0^*e^{i\omega t}}{\omega_{mn}+\omega-i0}\right].
\end{equation}

Accordingly,
\begin{equation}
  \begin{array}{rcl}
    \bar{x}&=&\displaystyle\int \Psi_n^*x\Psi_ndq\\\vbox to 20pt{}
    &=&\displaystyle\sum_m(a_mx_{nm}e^{i\omega_{nm}t}+
    a^*_{m}x_{mn}e^{i\omega_{mn}t})\\\vbox to 20pt{}
    &=&\displaystyle\frac{1}{2\hbar}\sum_mx_{mn}x_{nm}\left[
      \frac{1}{\omega_{mn}-\omega-i0}+\frac{1}{\omega_{mn}+\omega+i0}
      \right]f_0e^{-i\omega t}+\hbox{c.c},
  \end{array}
\end{equation}
it can be seen that
\begin{equation}
  \begin{array}{rcl}
    \alpha(\omega)&=&\displaystyle\frac{1}{\hbar}\sum_m|x_{mn}|^2\left[
      \frac{1}{\omega_{mn}-\omega-i0}+\frac{1}{\omega_{mn}+\omega+i0}
      \right]\\\vbox to 20pt{}
    &=&\displaystyle\frac{1}{\hbar}\sum_m|x_{mn}|^2\left[
      -\frac{1}{\omega_{nm}+\omega+i0}+\frac{1}{\omega_{mn}+\omega+i0}
      \right].
  \end{array}
\end{equation}
This expression is the Fourier transform of the function
\begin{equation}
  \alpha(t)=\frac{i}{\hbar}\theta(t)\average{x(t)x(0)-x(0)x(t)}=-G^R(t),
\end{equation}
thus the we have the final result
\begin{equation}
  \alpha(\omega)=\frac{i}{\hbar}\int_0^\infty e^{i\omega t}
  \average{x(t)x(0)-x(0)x(t)}dt.
\end{equation}

Similarly, if the generalized susceptibility of another physical
quantity $y$ is needed, we can write
\begin{equation}
  \begin{array}{rcl}
    \bar{y}&=&\displaystyle\int \Psi_n^*y\Psi_ndq\\\vbox to 20pt{}
    &=&\displaystyle \sum_m(a_my_{nm}e^{i\omega_{nm}t}+
    a^*_{m}y_{mn}e^{i\omega_{mn}t})\\\vbox to 20pt{}
    &=&\displaystyle\frac{1}{2\hbar}\sum_m\left[
      \frac{x_{mn}y_{nm}}{\omega_{mn}-\omega-i0}+
      \frac{x_{nm}y_{mn}}{\omega_{mn}+\omega+i0}
      \right]f_0e^{-i\omega t}+\hbox{c.c},
  \end{array}
\end{equation}
therefore
\begin{equation}
  \begin{array}{rcl}
    \alpha(\omega)&=&\displaystyle\frac{1}{\hbar}\sum_m\left[
      \frac{x_{mn}y_{nm}}{\omega_{mn}-\omega-i0}+
      \frac{x_{nm}y_{mn}}{\omega_{mn}+\omega+i0}
      \right]\\\vbox to 20pt{}
    &=&\displaystyle\frac{1}{\hbar}\sum_m\left[
      -\frac{x_{mn}y_{nm}}{\omega_{nm}+\omega+i0}+
      \frac{x_{nm}y_{mn}}{\omega_{mn}+\omega+i0}
      \right].
  \end{array}
\end{equation}
This expression is the Fourier transform of the function
\begin{equation}
  \alpha(t)=\frac{i}{\hbar}\theta(t)\sum_m[y_{nm}(t)x_{mn}-x_{nm}y_{mn}(t)].
\end{equation}
If the system in canonical distribution, then 
\begin{equation}
  \alpha(t)=\frac{i}{\hbar}\theta(t)\sum_{nm}\rho_n[y_{nm}(t)x_{mn}-x_{nm}y_{mn}(t)],
\end{equation}
or we can write in a more compact way
\begin{equation}
  \alpha(t)=\frac{i}{\hbar}\theta(t)\average{y(t)x(0)-x(0)y(t)}.
\end{equation}
Therefore
\begin{equation}
  \alpha(\omega)=\frac{i}{\hbar}\int_0^\infty e^{i\omega
    t}\average{y(t)x(0)-x(0)y(t)}dt.
\end{equation}

With the aid of Kubo identity $\alpha(t)$ can be written in another
form. Let $\rho=e^{-\beta H}/Z$, where $\beta=1/T$ and $Z$ is the
partition function, then
\begin{equation}
  \begin{array}{rcl}
    \alpha(t)&=&\displaystyle\frac{i}{\hbar}\theta(t)
    \average{y(t)x(0)-x(0)y(t)}\\\vbox to 20pt{}
    &=&\displaystyle\frac{i}{\hbar}\theta(t)
    \Tr[\rho\,y(t)x(0)-\rho\,x(0)y(t)]\\\vbox to 20pt{}
    &=&\displaystyle\frac{i}{\hbar}\theta(t)
    \Tr \Bigl[y(t)[x(0)\,\rho-\rho\,x(0)]\Bigr].
  \end{array}
\end{equation}
Substituting Kubo identity $[e^{-\beta H},x]=e^{-\beta H}\int_0^\beta
e^{\lambda H}[x,H]e^{-\lambda H}d\lambda$ into the above formula we
shall obtain that
\begin{equation}
  \begin{array}{rcl}
    \alpha(t)&=&\displaystyle\frac{i}{\hbar}\theta(t)\Tr\Bigl[y(t)\rho
    \int_0^\beta e^{\lambda H}[H,x(0)]e^{-\lambda H}d\lambda\Bigr]\\\vbox to 20pt{}
    &=&\displaystyle\frac{i}{\hbar}\theta(t)\Tr\Bigl[\rho
    \int_0^\beta e^{\lambda H}\dot{x}(0)e^{-\lambda H}y(t)d\lambda\Bigr]
    \\\vbox to 20pt{}
    &=&\displaystyle\frac{i}{\hbar}\theta(t)
    \int_0^\beta\left\langle e^{\lambda H}\dot{x}(0)e^{-\lambda H}
      y(t)\right\rangle d\lambda.
  \end{array}
\end{equation}
  

\chapter{Fluctuations}
\section{Gaussian Distribution}
The probability for a quantity $x$ to have a value in the interval
from $x$ to $x+dx$ is proportional to $e^{S(x)}$, where $S(x)$ is the
entropy formally regarded as a function of the exact value of $x$, namely
\begin{equation}
  w(x)={\rm constant}\times e^{S(x)}.
\end{equation}

The entropy $S$ has a maximum for $x=\bar{x}=0$. Hence 
$\partial S/\partial x=0$ and $\partial^2S/\partial x^2<0$ for $x=0$.
Expanding $S(x)$ in powers of $x$ and retaining only terms of up to the 
second order, we obtain
\begin{equation}
  S(x)=S(0)-\frac{1}{2}\beta x^2,
\end{equation}
where $\beta$ is a positive constant. Thus the probability distribution can be
written in the form
\begin{equation}
  w(x)=Ae^{-\frac{1}{2}\beta x^2}=\sqrt{\frac{\beta}{2\pi}}
  e^{-\frac{1}{2}\beta x^2}.
\end{equation}
The mean square fluctuation is 
\begin{equation}
  \average{x^2}=\int_{-\infty}^\infty x^2w(x)dx=\frac{1}{\beta}.
\end{equation}

In similar manner we can determine the probability of a simultaneous
deviation of several thermodynamic quantities from their mean values.
Let these deviations be denoted by $x_1,x_2,\cdots,x_n$. We define the
entropy $S(x_1,\cdots,x_n)$ as a function of the quantities
$x_1,\cdots,x_n$. Let $S$ be expanded in powers of the $x_i$, as far
as the second order terms, the difference $S-S_0$ is a
negative-definite quadratic form:
\begin{equation}
  S-S_0=-\frac{1}{2}\beta_{ik}x_ix_k,
\end{equation}
where $\beta_{ik}=\beta{ki}$. Thus the probability distribution can be
written as
\begin{equation}
  w=Ae^{-\frac{1}{2}\beta_{ik}x_ix_k}=\frac{\sqrt{\beta}}{(2\pi)^{\frac{n}{2}}}
    e^{-\frac{1}{2}\beta_{ik}x_ix_k},
\end{equation}
where $\beta$ is the determinant of $\beta_{ik}$.

Let us define the quantities
\begin{equation}
  X_i=-\frac{\partial S}{\partial x_i}=\beta_{ik}x_k,
\end{equation}
which we refer as thermodynamically conjugate to the $x_i$. According to 
Gaussian distribution properties, we find that
\begin{equation}
  \average{x_iX_k}=\delta_{ik},\quad\average{x_ix_k}=\beta^{-1}_{ik},\quad
  \average{X_iX_k}=\beta_{ik}.
\end{equation}

\section{Fluctuations of The Fundamental Thermodynamic quantities}
The probability $w$ of a fluctuations is proportional to $e^{S_t}$,
where $S_t$ is the total entropy of a closed system. We can equally
say that $w$ is proportional to $e^{\Delta S_t}$, where $\Delta S_t$
is the change in entropy in the fluctuation. Thus
\begin{equation}
  w\propto\exp\left(-\frac{\Delta E-T\Delta S+P\Delta V}{T}\right),
\end{equation}
where $\Delta E,\Delta S, \Delta V$ are the changes in the energy,
entropy and volume of the small part of the body in the fluctuation,
and $T,P$ the temperature and pressure of the medium.

Expanding $\Delta E$ in series, we obtain
\begin{equation}
  \Delta E-T\Delta S+P\Delta V=\frac{1}{2}\left[
    \frac{\partial^2E}{\partial S^2}(\Delta S)^2+
    2\frac{\partial^2E}{\partial S\partial V}\Delta S\Delta V+
    \frac{\partial^2E}{\partial V^2}(\Delta V)^2\right].
\end{equation}
It is easily seen that this expression may be written as
\begin{equation}
  \frac{1}{2}\left[
    \Delta S\Delta\left(\frac{\partial E}{\partial S}\right)_V+
    \Delta V\Delta\left(\frac{\partial E}{\partial V}\right)_S\right]
  =\frac{1}{2}(\Delta S\Delta T-\Delta P\Delta V).
\end{equation}
Thus we obtain the fluctuation probability in the form
\begin{equation}
  w\propto\exp\left(\frac{\Delta P\Delta V-\Delta T\Delta S}{2T}\right).
\end{equation}
From this general formula we can find the fluctuation of various
thermodynamic quantities. Let us take $V$ and $T$ as independent
variables, then
\begin{equation}
  \begin{array}{rcl}
    \Delta S&=&\displaystyle\left(\frac{\partial S}{\partial T}\right)_V\Delta T
    +\left(\frac{\partial S}{\partial V}\right)_T\Delta V=
    \frac{C_V}{T}\Delta T+\left(\frac{\partial P}{\partial T}\right)_V\Delta V,
    \\\vbox to 20pt{}
    \Delta P&=&\displaystyle\left(\frac{\partial P}{\partial T}\right)_V
    \Delta T+\left(\frac{\partial P}{\partial V}\right)_T\Delta V;
  \end{array}
\end{equation}
therefore the distribution function becomes
\begin{equation}
  w\propto\exp\left[-\frac{C_V}{2T^2}(\Delta T)^2+\frac{1}{2T}
    \left(\frac{\partial P}{\partial V}\right)_T(\Delta V)^2\right].
\end{equation}
Applying the general formula for the Gaussian distribution, we find
the following expressions for the mean square fluctuations of temperature
and volume:
\begin{equation}
  \average{\Delta T\Delta V}=0,\quad
  \average{(\Delta T)^2}=\frac{T^2}{C_V},\quad
  \average{(\Delta V)^2}=-T\left(\frac{\partial V}{\partial P}\right)_T.
\end{equation}
These quantities are positive by virtue of the thermodynamic inequalities 
$C_V>0$ and $(\partial P/\partial V)_T<0$.

Let us now take $P$ and $S$ as the independent variables, then
\begin{equation}
  \begin{array}{rcl}
    \Delta V&=&\displaystyle\left(\frac{\partial V}{\partial P}\right)_S
    \Delta P+\left(\frac{\partial V}{\partial S}\right)_P\Delta S=
    \left(\frac{\partial V}{\partial P}\right)_S\Delta P+
    \left(\frac{\partial T}{\partial P}\right)_S\Delta S;
    \\\vbox to 20pt{}
    \Delta T&=&\displaystyle\left(\frac{\partial T}{\partial S}\right)_P
    \Delta S+\left(\frac{\partial T}{\partial P}\right)_S\Delta P=
    \frac{T}{C_P}\Delta S+\left(\frac{\partial T}{\partial P}\right)_S
    \Delta P.
  \end{array}
\end{equation}
Therefore the distribution function becomes
\begin{equation}
  w\propto\exp\left[
    \frac{1}{2T}\left(\frac{\partial V}{\partial P}\right)_S(\Delta P)^2-
    \frac{1}{2C_P}(\Delta S)^2\right],
\end{equation}
and the mean square fluctuations are
\begin{equation}
  \average{\Delta S\Delta P}=0,\quad
  \average{(\Delta S)^2}=C_P,\quad
  \average{(\Delta P)^2}=-T\left(\frac{\partial P}{\partial V}\right)_S.
\end{equation}

Since $\average{(\Delta V)^2}=-T(\partial V/\partial P)_T$, dividing
both sides by $N^2$ we find the volume fluctuation per particle:
\begin{equation}
  \average{(\Delta (V/N))^2}=-\frac{T}{N^2}\left(\frac{\partial
    V}{\partial P}\right)_T.
\end{equation}
We can find the fluctuation of the number of particles in a fixed
volume. Since $V$ is then constant, we must put
$\Delta(V/N)=V\Delta(1/N)=-(V/N^2)\Delta N$, then
\begin{equation}
  \average{(\Delta N)^2}=-T\frac{N^2}{V^2}\left(\frac{\partial V}{\partial P}
  \right)_T.
\end{equation}
Since $(\partial V\partial P)_T$ is regarded as taken with $N$ constant, we
write
\begin{equation}
  -\frac{N^2}{V^2}\left(\frac{\partial V}{\partial P}\right)_{T,N}=
  N\left(\frac{\partial}{\partial P}\frac{N}{V}\right).
\end{equation}
The function $N/V$ is a function of $P$ and $T$ only, and therefore does not
matter whether $N/V$ is differentiated at constant $N$ or constant $V$, hence
we can write
\begin{equation}
  N\left(\frac{\partial}{\partial P}\frac{N}{V}\right)_{T,N}=
  \frac{N}{V}\left(\frac{\partial N}{\partial P}\right)_{T,V}=
  \left(\frac{\partial N}{\partial P}\right)_{T,V}
  \left(\frac{\partial P}{\partial\mu}\right)_{T,V}=
  \left(\frac{\partial N}{\partial\mu}\right)_{T,V},
\end{equation}
where we have used the equation $N/V=(\partial P/\partial\mu)_{T,V}$,
which follows from formula $d\Omega=-VdP=-SdT-Nd\mu$. Thus we have for
the fluctuation of the number of particles the formula
\begin{equation}
  \average{(\Delta N)^2}=T\left(\frac{\partial N}{\partial \mu}\right)_{T,V}.
\end{equation}
For ideal gas, substituting $PV=NT$ gives
\begin{equation}
  \average{(\Delta N)^2}=N.
\end{equation}

Let us consider an assembly of $n_k$ particles in the $k$th quantum state, 
then
\begin{equation}
  \average{(\Delta n_k)^2}=T\frac{\partial\bar{n}_k}{\partial\mu}.
\end{equation}
For a Fermi gas we must substitute
\begin{equation}
  \bar{n}_k=\frac{1}{e^{(\varepsilon_k-\mu)/T}+1},
\end{equation}
the differentiation gives
\begin{equation}
  \average{(\Delta n_k)^2}=\bar{n}_k(1-\bar{n}_k).
\end{equation}
Similarly, for a Bose gas
\begin{equation}
  \average{(\Delta n_k)^2}=\bar{n}_k(1+\bar{n}_k).
\end{equation}
For a Boltzmann gas the substitution $\bar{n}_k=e^{(\mu-\varepsilon_k)/T}$ gives
\begin{equation}
  \average{(\Delta n_k)^2}=\bar{n}_k,
\end{equation}
where $\bar{n}_k\ll1$.




\section{Correlations of Fluctuations in Time}
There is some correlation between the values of $x(t)$ at different instants,
we define this correlation as
\begin{equation}
  \phi(t-t')=\average{x(t)x(t')}.
\end{equation}
This correlation depends only on the difference $t-t'$, and the
definition may therefore also be written
\begin{equation}
  \phi(t)=\average{x(t)x(0)}.
\end{equation}
Note also that, because of the obvious symmetry of the definition as
regards the interchange of $t$ and $t'$, the function $\phi(t)$ is even:
\begin{equation}
  \phi(t)=\phi(-t).
\end{equation}
The definition given above can be put in a form that is applicable to
quantum variables also. To do this, we must consider in place of
quantity $x$ its Heisenberg operator $x(t)$. The operators $x(t)$ and
$x(t')$ relating to different instants do not in general commute, and
the correlation function must now be defined as
\begin{equation}
  \phi(t-t')=\frac{1}{2}\average{x(t)x(t')+x(t')x(t)}.
\end{equation}

Let the quantity $x$ have at some instants a value which is large
compared with the mean fluctuation, i.e. the system be far from
equilibrium. Then we can say that at subsequent instants the system
will tend to reach equilibrium state. Under the assumption made, its
rate of change will be at every instant entirely defined by the value
of $x$ at that instant: $\dot{x}=\dot{x}(x)$. Expanding $\dot{x}$ in powers of
$x$, keeping only the linear term:
\begin{equation}
  \frac{dx}{dt}=-\lambda x.
\end{equation}

Returning to fluctuations in an equilibrium system, let us define a
quantity $\xi_x(t)$ as the mean value of $x$ at an instant $t>0$ with
the condition that it had some given value $x$ at the prior instant
$t=0$. Evidently the correlation function $\phi(t)$ may be written in 
terms of $\xi_x(t)$ as
\begin{equation}
  \phi(t)=\average{x\xi_x(t)},
\end{equation}
where the averaging is only over the probabilities of the various
values of $x$ at the initial instant $t=0$. For $\xi_x(t)$ is an
averaged quantity, we must expect that
\begin{equation}
\frac{d\xi_x(t)}{dt}=-\lambda\xi_x(t),\quad t > 0
\end{equation}
is true even when $\xi_x(t)$ is not large. Since $\xi_x(0)=x$ by
definition, we have that
\begin{equation}
  \xi_x(t)=xe^{-\lambda t},
\end{equation}
and finally we obtain a formula for the time correlation function
\begin{equation}
  \phi(t)=\average{x^2}e^{-\lambda t}.
\end{equation}
On the other hand, since $\phi(t)$ is an even function, we can write
the final formula as
\begin{equation}
  \phi(t)=\average{x^2}e^{-\lambda |t|}=\frac{1}{\beta}e^{-\lambda|t|}.
\end{equation}

The above theory can be also be formulated in another way that may
have certain advantages. The equation $\dot{x}=-\lambda x$ is valid
only when $x$ is large compared with the mean fluctuation of $x$.  For
arbitrary values of $x$, we write
\begin{equation}
  \label{eq:random-force}
  \dot{x}=-\lambda x+y,
\end{equation}
where $y$ is the random force. The magnitude of the oscillations of
$y$ does not change with time, when $x$ is large $y$ is relatively
small and may be neglected. The correlation function of the random
force, $\average{y(t)y(0)}$, must be specified in such a way as to
lead the correct result for $\average{x(t)x(0)}$. To do so, we must 
put 
\begin{equation}
  \average{y(t)y(0)}=2\lambda\average{x^2}\delta(t)=
  \frac{2\lambda}{\beta}\delta(t).
\end{equation}
This is easily seen by writing the solution of equation
\eqref{eq:random-force}
\begin{equation}
  x(t)=e^{-\lambda t}\int_{-\infty}^{t}y(\tau)e^{\lambda\tau}d\tau,
\end{equation}
and averaging the product $x(t)x(0)$ after expressing it as a
double integral.

If there are several quantities $x_1,\cdots,x_n$ simultaneously
deviate from their equilibrium values. The correlation functions for
the fluctuations of these quantities are defined (in the classical
theory) as
\begin{equation}
  \phi_{ik}(t-t')=\average{x_i(t)x_k(t')}.
\end{equation}
By virtue of this definition, they have the obvious symmetry property
\begin{equation}
  \phi_{ik}(t)=\phi_{ki}(-t).
\end{equation}
Since there is also time reversal symmetry, there is
\begin{equation}
  \phi_{ik}(t)=\phi_{ik}(-t),
\end{equation}
combining these two symmetries we obtain that
\begin{equation}
  \phi_{ik}(t)=\phi_{ki}(t).
\end{equation}

The equations for $\dot{x}_i$ now become
\begin{equation}
  \dot{x}_i=-\lambda_{ik}x_k
\end{equation}
with constant coefficients $\lambda_{ik}$. We define the mean values
$\xi_i(t)$ of the quantities $x_i$ at a time $t>0$ for given values of
all the $x_1,\cdots,x_n$ at the earlier time $t=0$. These quantities
satisfy the equation
\begin{equation}
  \dot{\xi_i}=-\lambda_{ik}\xi_k.
\end{equation}
Now the equation for correlation functions can be written as
\begin{equation}
  \frac{d\phi_{il}(t)}{dt}=\frac{d\average{\xi_l(t)x_i}}{dt}=
    -\lambda_{ik}\phi_{kl}(t),
\end{equation}
with the initial conditions
\begin{equation}
  \phi_{ik}(0)=\average{x_ix_k}=\beta^{-1}_{ik}.
\end{equation}

\section{Onsager's Principle}
Let us return to the macroscopic equations
\begin{equation}
  \dot{x}_i=-\lambda_{ik}x_k.
\end{equation}
These equations have a deep-lying internal symmetry, which becomes
explicit only when the right-hand sides are expressed in terms of the 
thermodynamically conjugate quantities
\begin{equation}
  X_i=-\frac{\partial S}{\partial x_i}=\beta_{ik}x_k,
\end{equation}
with $\beta_{ik}=\beta_{ki}$. 

If we express the quantities $\dot{x}_i$ in terms of $X_i$, we obtain
the relaxation equations in the form
\begin{equation}
  \dot{x}_i=-\gamma_{ik}X_k,
\end{equation}
where
\begin{equation}
  \gamma_{ik}=\lambda_{il}\beta^{-1}_{lk}
\end{equation}
are new constants called {\it kinetic coefficients}. We shall prove
the {\it principle of the symmetry of the kinetic coefficients} or
{\it Onsager's principle}, according to which 
\begin{equation}
  \gamma_{ik}=\gamma_{ki}.
\end{equation}

We define mean values $\xi_i(t)$ of the fluctuations quantities $x_i$,
and mean values $\varXi_i(t)$ of the $X_i$, then
\begin{equation}
  \dot{\xi_i}=-\gamma_{ik}\varXi_k.\quad (t>0)
\end{equation}
We now make use of the symmetry $\phi_{ik}(t)=\phi_{ik}(-t)$, which
may be written 
\begin{equation}
  \average{x_i(t)x_k(0)}=\average{x_i(0)x_k(t)},
\end{equation}
or, with $\xi_i(t)$
\begin{equation}
  \average{\xi_i(t)x_k}=\average{x_i\xi_k(t)}.
\end{equation}
Differentiate this equation with respect to $t$ and we obtain that
\begin{equation}
  \gamma_{il}\average{\varXi_l x_k}=\gamma_{kl}\average{x_i\varXi_l}.
\end{equation}
Putting $t=0$ in the above equation, we get
\begin{equation}
  \gamma_{il}\average{X_lx_k}=\gamma_{kl}\average{x_iX_l},
\end{equation}
since $\average{x_iX_k}=\average{X_kx_i}=\delta_{ik}$ the final result
is arrived.

It has been assumed in the derivation that the quantities $x_i$ and
$x_k$ are unaffected by time reversal. The relation remains valid if
both quantities change sign under time reversal. But if one of $x_i$
and $x_k$ changes sign and the other remains unchanged\footnote{For instance, 
one is the velocity $v=\dot{x}$ and the other is $x$.}, the principle
of the symmetry of the kinetic coefficients is formulated as
\begin{equation}
  \gamma_{ik}=-\gamma_{ki}.
\end{equation}

Exactly similar results are valid for kinetic coefficients
$\zeta_{ik}=\zeta_{ki}$ which appear in the relaxation equations when these are
put in the form thermodynamically conjugate to equations:
\begin{equation}
  \dot{X}_i=-\zeta_{ik}x_k,\quad \zeta_{ik}=\beta_{ik}\lambda_{kl}.
\end{equation}

\section{Spectral Resolution of Fluctuations}
The spectral resolution of a fluctuating quantity $x(t)$ is defined by
the usual Fourier expansion formula:
\begin{equation}
  x_\omega=\int_{-\infty}^{\infty} x(t)e^{i\omega t}dt,
\end{equation}
and conversely 
\begin{equation}
  x(t)=\int_{-\infty}^{\infty}x_\omega e^{-i\omega t}\frac{d\omega}{2\pi}.
\end{equation}
And the spectral resolution of correlation function $\phi(t)$ is
defined as
\begin{equation}
  \phi(t)=\int_{-\infty}^{\infty}(x^2)_\omega\frac{\omega}{2\pi},\quad
  (x^2)_\omega=\int_{-\infty}^{\infty} \phi(t)e^{i\omega t}dt,
\end{equation}
and the spectral resolution of $\phi(t-t')$ is
\begin{equation}
  \phi(t-t')=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\average{x_{\omega}x_{\omega'}}
  e^{-i(\omega t+\omega't')}\frac{d\omega d\omega'}{(2\pi)^2},
\end{equation}
comparing the above two equations we obtain that
\begin{equation}
  \average{x_{\omega}x_{\omega'}}=2\pi(x^2)_\omega\delta(\omega+\omega').
\end{equation}

Since $\phi(t)=(1/\beta)e^{-\lambda|t|}$, an integration of it would gives
\begin{equation}
  (x^2)_\omega=\frac{1}{\beta}\left[\frac{1}{\lambda-i\omega}+
    \frac{1}{\lambda+i\omega}\right]=\frac{2\lambda}{\beta(\omega^2+\lambda^2)}.
\end{equation}
In terms of the random force $y(t)$, the equation is $\dot{x}=-\lambda
x+y$. Multiplying by $e^{i\omega t}$ and integrating with respect to
$t$ from $-\infty$ to $\infty$ (the term $\dot{x}e^{i\omega t}$ being
integrated by parts), we obtain
$(\lambda-i\omega)x_\omega=y_\omega$. Therefore
\begin{equation}
  2\pi(y^2)_\omega\delta(\omega+\omega')=\average{y_\omega y_{\omega'}}=
  (\lambda-i\omega)(\lambda-i\omega')\average{x_\omega x_{\omega'}}=
  2\pi(\omega^2+\lambda^2)(x^2)_\omega\delta(\omega+\omega'),
\end{equation}
i.e.,
\begin{equation}
  (y^2)_\omega=(\omega^2+\lambda^2)(x^2)_\omega=\frac{2\lambda}{\beta}.
\end{equation}

The above expression can also be generalized to the simultaneous
fluctuations of several thermodynamic variables. The components of
their spectral resolution are
\begin{equation}
  (x_ix_k)_\omega=\int_{-\infty}^{\infty}\phi_{ik}(t)e^{i\omega t}dt\equiv
  \int_{-\infty}^{\infty}\average{x_i(t)x_k(0)}e^{i\omega t}dt,
\end{equation}
and we have 
\begin{equation}
  \average{x_{i\omega}x_{k\omega'}}=2\pi(x_ix_k)_\omega\delta(\omega+\omega').
\end{equation}
A change in the sign of the time is equivalent to the change
$\omega\to-\omega$ in the spectral resolution, which in tern implies
taking the complex conjugate of $(x_ix_k)_\omega$. The symmetry
$\phi_{ik}(t)=\phi_{ki}(-t)$ shows that
\begin{equation}
  (x_ix_k)_\omega=(x_kx_i)_{-\omega}=(x_kx_i)_\omega^*.
\end{equation}
The time reversal symmetry, $\phi_{ik}(t)=\phi_{ik}(-t)$ or
$\phi_{ik}(t)=-\phi_{ik}(-t)$, is written in terms of the spectral
resolution as
\begin{equation}
  (x_ix_k)_\omega=\pm(x_ix_k)_{-\omega}=\pm(x_ix_k)_\omega^*,
\end{equation}
where the $+$ and $-$ signs respectively relate to cases where $x_i$
and $x_k$ behave similarly or differently under time reversal; in the
former case, $(x_ix_k)_\omega$ is real and symmetrical in the suffixes
$i$ and $k$, while in the latter case it is imaginary and
antisymmetrical.

The equation for $\phi_{ik}$ is 
\begin{equation}
  \frac{d\phi_{il}(t)}{dt}=-\lambda_{ik}\phi_{kl}(t),
\end{equation}
after the Fourier transform (integrate respect to $t$ from $0$ to
$\infty$) it becomes
\begin{equation}
  -\phi_{il}(0)-i\omega(x_ix_l)^{(+)}_\omega=-\lambda_{ik}(x_kx_l)^{(+)}_\omega,
\end{equation}
with the notation
\begin{equation}
  (x_kx_l)^{(+)}_\omega=\int_0^\infty\phi_{il}(t)e^{i\omega t}dt.
\end{equation}
Since $\phi_{ik}(0)=\beta^{-1}_{ik}$, we obtain that
\begin{equation}
  (x_ix_k)_\omega=(x_ix_k)^{(+)}_\omega+(x_kx_i)_\omega^{(+)*}=
  (\zeta-i\omega\beta)^{-1}_{ik}+(\zeta+i\omega\beta)^{-1}_{ki},
\end{equation}
where $\zeta_{ik}=\beta_{il}\lambda_{lk}$.

If we use random force to formulate the theory, then 
\begin{equation}
  \dot{x}_i=-\lambda_{ik}x_k+y_i,
\end{equation}
after the Fourier transform, it becomes
\begin{equation}
  (\lambda_{ik}-i\omega\delta_{ik})x_{k\omega}=y_{i\omega}.
\end{equation}
Then finally we get the formula
\begin{equation}
  \begin{array}{rcl}
  (y_iy_k)_\omega&=&(\lambda_{il}-i\omega\delta_{il})(x_lx_m)_\omega
  (\lambda_{km}-i\omega\delta_{km})\\\vbox to 20pt{}
  &=&\gamma_{ik}+\gamma_{ki},
  \end{array}
\end{equation}
where $\gamma_{ik}=\lambda_{il}\beta^{-1}_{lk}.$

For a quantum variable, the spectral density $(x^2)_\omega$ is defined by
\begin{equation}
  \frac{1}{2}\average{x_\omega x_{\omega'}+x_{\omega'}x_\omega}=
  2\pi(x^2)_\omega\delta(\omega+\omega').
\end{equation}
According to fluctuation dissipation theorem, 
\begin{equation}
  (x^2)_\omega=\hbar\Im\,\alpha(\omega)\coth\frac{\hbar\omega}{2T},
\end{equation}
and the mean square of the fluctuating quantity is given by the
integration
\begin{equation}
  \average{x^2}=\frac{\hbar}{\pi}\int_0^\infty\Im\,\alpha(\omega)
  \coth\frac{\hbar\omega}{2T}d\omega.
\end{equation}

At temperature $T\gg\hbar\omega$, we have
$\coth(\hbar\omega/2T)\approx2T/\hbar\omega$, then
\begin{equation}
  (x^2)_\omega=(2T/\omega)\Im\alpha(\omega),
\end{equation}
and (using Kramers-Kronig relation here)
\begin{equation}
  \average{x^2}=\frac{2T}{\pi}\int_0^\infty
  \frac{\Im\alpha(\omega)}{\omega}d\omega=
  T\alpha(0).
\end{equation}
If the body is subject to the action of a static force $f$, there
would be a displacement $\alpha(0)f=f/\beta T$ of $\hbar{x}$. The
macroscopic equation of the relaxation then have the form 
\begin{equation}
  \dot{x}=-\lambda(x-f/\beta T).
\end{equation}

Now let $f$ is a time-dependent perturbation $f(t)$, then we obtain that
\begin{equation}
  -i\omega\alpha(\omega)f_0=\lambda\alpha(\omega)f_0+(\lambda/\beta T)f_0,
\end{equation}
whence 
\begin{equation}
  \alpha(\omega)=\frac{\lambda}{\beta T(\lambda-i\omega)}.
\end{equation}
Therefore $(x^2)_\omega$ becomes
\begin{equation}
  (x^2)_\omega=\frac{2\lambda}{\beta(\lambda^2+\omega^2)}
  \frac{\hbar\omega}{2T}\coth\frac{\hbar\omega}{2T},
\end{equation}
this expression differs from the classical one by a extra factor
$\frac{\hbar\omega}{2T}\coth\frac{\hbar\omega}{2T}$. 
We can also formulate it with the random force $y=\lambda f/\beta T$, then 
\begin{equation}
  (y^2)_\omega=\frac{2\lambda}{\beta}\frac{\hbar\omega}{2T}
  \coth\frac{\hbar\omega}{2T},
\end{equation}
which differs from the classical one by the same factor. For several
thermodynamic quantities
\begin{equation}
  (y_iy_k)_\omega=(\gamma_{ik}+\gamma_{ki})\frac{\hbar\omega}{2T}
  \coth\frac{\hbar\omega}{2T}.
\end{equation}

\subsection{Fluctuation of a One Dimensional oscillator}
As an example of the use of the above formula, let us consider
fluctuations of a one dimensional oscillator. We write its Hamiltonian
in the form $H=\frac{P^2}{2m}+\frac{1}{2}m\omega_0Q^2$. Then the distribution
function for $Q$ is
\begin{equation}
  \sqrt{\frac{\beta}{2\pi}}e^{-\frac{1}{2}\beta x^2}=
  \sqrt{\frac{m\omega_0^2}{2\pi T}}e^{-\frac{1}{2}\frac{m\omega_0^2Q^2}{T}},
\end{equation}
and the mean square fluctuation is
\begin{equation}
  \average{Q^2}=\frac{T}{m\omega_0^2}.
\end{equation}

The equation of motion of an oscillator with friction and random force are
\begin{equation}
  \dot{Q}=\frac{P}{m},\quad \dot{P}=-m\omega_0^2Q-\gamma\frac{P}{m}+y,
\end{equation}
represent the relations $\dot{x}_i=-\gamma_{ik}X_k$, so that
\begin{equation}
  \gamma_{11}=0,\quad \gamma_{12}=-\gamma_{21}=-T,\quad\gamma_{22}=\gamma T,
\end{equation}
note that $P$ is antisymmetrical under time reversal.  Therefore we
can write the spectral density of the fluctuations of the random force
\begin{equation}
  (y^2)_\omega=\gamma_{22}+\gamma_{22}=2\gamma T.
\end{equation}

Because $P=m\dot{Q}$, we have that
\begin{equation}
  m\ddot{Q}+\gamma\dot{Q}+m\omega_0^2Q=y,
\end{equation}
after Fourier transform it becomes
\begin{equation}
  (-m\omega^2-i\omega\gamma+m\omega_0^2)Q_\omega=y_\omega,
\end{equation}
and hence finally 
\begin{equation}
  (Q^2)_\omega=\frac{2\gamma T}{m^2(\omega^2-\omega_0^2)^2+\omega^2\gamma^2}.
\end{equation}

\section{Thermoelectric Fluctuations}
A particle current can be induced by external electric field and
temperature gradient, the formula is
\begin{equation}
  j=\sigma E-S\nabla T,
\end{equation}
where $S$ is the Seebeck coefficient (also known as the thermopower).

Similarly, the formula for heat current is
\begin{equation}
  q-\mu j=\beta E-k\nabla T.
\end{equation}

An electric field does mechanical work on the current-carrying
particles, the work done per unit time and volume is equal to the
scalar product $j\cdot E$. This is ``Joule's law''. The evolution of
heat results in an increase in the entropy of the body. When an amount
of heat $dQ=j\cdot EdV$ is involved, the entropy increases by $dQ/T$.
The rate of change of the total entropy is therefore
\begin{equation}
  \frac{ds}{dt}=\int\frac{j\cdot E}{T}dV.
\end{equation}
And the entropy change due to heat current is
\begin{equation}
  \frac{ds}{dt}=-\int\frac{\nabla(q-\mu j)}{T}dV=
  -\int\frac{(q-\mu j)\cdot\nabla T}{T^2}dV.
\end{equation}
Therefore the total entropy change is
\begin{equation}
  \frac{ds}{dt}=\int\frac{j\cdot E}{T}dV-\int\frac{(q-\mu j)\nabla T}{T^2}dV.
\end{equation}

This formula can be derived in another way, writing the entropy change as
$ds/dt=-\int\nabla q/TdV$ and put $\nabla j=0$ then we have
\begin{equation}
  \frac{\nabla q}{T}=\frac{1}{T}[\nabla(q-\mu j)+j\nabla\mu]=
  \frac{1}{T}\nabla(q-\mu j)-\frac{j\cdot E}{T},
\end{equation}
finally the same formula is obtained.

Let $\dot{x}_i$ be $j$ and $q-\mu j$, we find that $X_i$ are $-E/T$
and $\nabla T/T^2$. Accordingly in the relations
\begin{equation}
  j=\sigma T\frac{E}{T}-ST^2\frac{\nabla T}{T^2},\quad
  q-\mu j=\beta T\frac{E}{T}-kT^2\frac{\nabla T}{T^2}.
\end{equation}
Let us label $j$ as 1 and $q-\mu j$ as 2, then
\begin{equation}
  \gamma_{11}=\sigma T,\quad \gamma_{12}=-ST^2,\quad
  \gamma_{21}=\beta T,\quad \gamma_{22}=-kT^2.
\end{equation}
According to Onsager's principle, $\gamma_{12}=\gamma_{21}$.  If we
write the equation of motion in form of random force, 
\begin{equation}
  \dot{x}_a=-\sum_b\gamma_{ab}X_b+y_a,
\end{equation}
the coefficients $\gamma_{ab}$ is immediately given by
\begin{equation}
  \average{y_a(t_a)y_b(t_b)}=(\gamma_{ab}+\gamma_{ba})\delta(t_a-t_b),\quad
  {\rm or}\quad(y_ay_b)_\omega=\gamma_{ab}+\gamma_{ba}.
\end{equation}

\chapter{Coherent Potential Approximation}
Perfect lattice have been studied intensively by physicist since the
beginning of quantum mechanics. The periodicity of the lattice allows
us to use Bloch waves to describe the wave function of electrons in
lattice. However, in real world the perfect lattice does not exist and
there are always some kinds of disorders in lattice. Usually weak
disorder limit is described by the scattering of Bloch waves by
impurities, and Boltzmann transport equation is enough to handle
it. But in strong disorder limit, the situation becomes much more
complicated. In this chapter we shall briefly review methods to deal
with disordered electronic systems in lattice.

\section{The Hamiltonian for Disordered Electronic Systems}
In lattice, we use an unified tight-binding Hamiltonian to describe
the disorders:
\begin{equation}
  \label{eq:disordered-hamiltonian}
  \hat{H}=\sum_{ij}W_{ij}\hat{c}^\dag_i\hat{c}_j
  +\sum_{i}V_i\hat{c}^\dagger_i\hat{c}_i,
\end{equation}
where $\hat{c}_i^\dagger$ ($\hat{c}_i$) are creation (annihilation)
operators of electron at site $i$, and $V_i$ is the on-site energy
when there is an electron at site $i$. The quantity $W_{ij}$ describe
the hopping energy for transition of an electron from site $j$ to site
$i$, and hence $W_{ij}=0$ when $i=j$. It is clear that $V_i$ are
diagonal elements of the Hamiltonian, while $W_{ij}$ are off diagonal
elements. If the diagonal elements $V_i$ are random variables, then we
call the system diagonal disorder system and the off diagonal elements
$W_{ij}$ is still transitional invariance. On the contrary, if $V_i$
is a position independent constant while $W_{ij}$ are random
variables, such a system is called off diagonal disorder system.

In this chapter we shall focus on the diagonal disorder systems. First
we split the Hamiltonian as
\begin{equation}
  \hat{H}=\hat{H}_0+\hat{H}_1,
\end{equation}
where the off diagonal elements
\begin{equation}
  \hat{H}_0=\sum_{ij}W_{ij}\hat{c}^\dagger_i\hat{c}_j
\end{equation}
are the ordered terms in the Hamiltonian, while the diagonal elements
\begin{equation}
  \hat{H}_1=\sum_{i}V_{i}\hat{c}_i^\dagger\hat{c}_i
\end{equation}
are disordered terms. This corresponds to the random potential. Since
the hopping terms $W_{ij}$ are ordered terms, $\hat{H}_0$ is just the
usual Hamiltonian of tight binding model which can be diagonalized by
Fourier transformation to $\bm{k}$-representation. Therefore sometimes
$\hat{H}_0$ is also written as
\begin{equation}
  \hat{H}_0=\sum_{\bm{k}}\varepsilon_{\bm{k}}\hat{c}^\dagger_{\bm{k}}\hat{c}_{\bm{k}},
\end{equation}
where $\varepsilon_{\bm{k}}$ is dispersion relation of the energy
band. 

\subsection*{The Green's Function}
The single particle Green's function in disordered system is defined
as the retarded Green's function with respect to vacuum state 
\begin{equation}
  G_{ij}(t)=-\frac{i}{\hbar}\theta(t)
  \bra{0}\hat{c}_i(t)\hat{c}^\dagger_j(0)+\hat{c}^\dagger_j(0)\hat{c}_i(t)\ket{0},
\end{equation}
where $\ket{0}$ is the vacuum state. Because the Green's function is
defined with respect to vacuum state, it is clear that
\begin{equation}
  \bra{0}\hat{c}_j^\dagger(0)\hat{c}_i(t)\ket{0}=0.
\end{equation}
Therefore the Green's function reduces to
\begin{equation}
  G_{ij}(t)=-\frac{i}{\hbar}\bra{0}\hat{c}_i(t)\hat{c}^\dagger_j(0)\ket{0},
  \quad t \ge 0.
\end{equation}

The physical meaning of the Green's function $G_{ij}(t)$ is clear: it
creates an electron at site $j$ then annihilates an electron after
time $t$ at site $i$, and gives the coefficient amplitude of an
electron at site $i$. Thus the square modulus $|G_{ij}(t)|^2$ gives
the probability of finding the electron at site $i$ at time $t$ with
its initial position at site $j$ at time $0$. In other words, the
Green's function acts as a propagator. 

Now we move to the equation of motion of the Green's
function. According to Heisenberg equation we have
\begin{equation}
  i\hbar\frac{d\hat{c}_i(t)}{dt}=[\hat{c}_i(t),\hat{H}],
\end{equation}
and substituting \eqref{eq:disordered-hamiltonian} in it and
collaborating the commutation rule for fermions
$\hat{c}_i\hat{c}_j^\dagger+\hat{c}_j^\dagger\hat{c}_i=\delta_{ij}$ we
obtain
\begin{equation}
  i\hbar\frac{d\hat{c}_i(t)}{dt}=V_i\hat{c}_i(t)+\sum_{k}W_{ik}\hat{c}_k(t).
\end{equation}
Therefore the equation of motion of the Green's function is
\begin{equation}
  \begin{array}{rcl}
    \displaystyle\frac{dG_{ij}(t)}{dt}&=&\displaystyle(-i/\hbar)^2
    \bra{0}[\hat{c}_i(t),\hat{H}]\hat{c}_j^\dagger(0)\ket{0}
    -\frac{i}{\hbar}\delta_{ij}(t)\\\vbox to 18pt{}
    &=&\displaystyle(-i/\hbar)^2[\bra{0}V_i\hat{c}_i(t)\hat{c}_j^\dagger(0)\ket{0}
    +\sum_{k}\bra{0}W_{ik}\hat{c}_k(t)c^\dagger_j(0)\ket{0}]-
    \frac{i}{\hbar}\delta_{ij}(t),
  \end{array}
\end{equation}
where the delta function $\delta_{ij}(t)$ comes from the discontinuity
of the Green's function at time $t=0$, thus we have for $t\ge0$
\begin{equation}
  i\hbar\frac{dG_{ij}(t)}{dt}=V_iG_{ij}(t)+\sum_{k}W_{ik}G_{kj}(t)+\delta_{ij}(t).
\end{equation}
Apply Fourier transform on both sides we have the equation in energy
representation as
\begin{equation}
  \label{eq:equation-motion}
  EG_{ij}(E)=V_iG_{ij}(E)+\sum_kW_{ik}G_{kj}(E)+\delta_{ij},
\end{equation}
where, since $G_{ij}(t)=0$ when $t<0$,
\begin{equation}
  G_{ij}(E)=\int_{-\infty}^\infty G_{ij}(t)e^{\frac{i}{\hbar}Et}dt=
  \int_0^\infty G_{ij}(t)e^{\frac{i}{\hbar}Et}dt,
\end{equation}
and the inverse Fourier transform is
\begin{equation}
  G_{ij}(t)=\int_{-\infty}^\infty G_{ij}(E)e^{-\frac{i}{\hbar}Et}\frac{dE}{2\pi\hbar}.
\end{equation}
The equation \eqref{eq:equation-motion} can be arranged as
\begin{equation}
  \label{eq:equation-motion-1}
  \sum_k[(E-V_i)\delta_{ik}-W_{ik}]G_{kj}(E)=\delta_{ij}.
\end{equation}
Now use notation $\hat{G}$ to denote the matrix $G_{ij}(E)$, $\hat{W}$
to denote the matrix $W_{ij}$ and $\hat{V}$ to denote the matrix
$V_i\delta_{ij}$. And write the identity matrix as $\hat{I}$. Thus the
Hamiltonian can be written as $\hat{H}=\hat{W}+\hat{V}$ and the
equation \eqref{eq:equation-motion-1} can be written as
\begin{equation}
  \label{eq:equation-motion-2}
  (E\hat{I}-\hat{V}-\hat{W})\hat{G}(E)=\hat{I}.
\end{equation}
Hence the formal solution for the Green's function is written as
\begin{equation}
  \label{eq:equation-motion-solution}
  \hat{G}(E)=(E\hat{I}-\hat{H})^{-1}=(E\hat{I}-\hat{V}-\hat{W})^{-1}.
\end{equation}
Now define the unperturbed Green's function as
\begin{equation}
  \hat{G}_0(E)=(E\hat{I}-\hat{H}_0)^{-1}=(E\hat{I}-\hat{W})^{-1}, 
\end{equation}
then \eqref{eq:equation-motion-2} may be written 
\begin{equation}
  (\hat{G}_0^{-1}-\hat{V})\hat{G}=\hat{I},
\end{equation}
or
\begin{equation}
  \label{eq:disordered-dyson}
  \hat{G}=\hat{G}_0+\hat{G}_0\hat{V}\hat{G}.
\end{equation}
This is the required equation of motion of the Green's function in
energy representation and it is just the Dyson equation.

\section{Average $T$-Matrix Approximation}
Since the system is disordered, it is impossible to obtain the exact
Green's function. However, the statistical properties of the system
are related to the average of the disorders, thus the studies of the
disordered system mainly focus on the average of the Green's function
with respect to all possible realization of disorders. In this section
we shall introduce a method called the average $T$-matrix
approximation to calculate the average Green's function.

Substituting \eqref{eq:disordered-dyson} into the right side of itself
iteratively we obtain an infinite series expression for $\hat{G}$ that
\begin{equation}
  \label{eq:disordered-dyson-1}
  \hat{G}=\hat{G}_0+\hat{G}_0\hat{V}\hat{G}_0+
  \hat{G}_0\hat{V}\hat{G}_0\hat{V}\hat{G}_0+\cdots.
\end{equation}
Now define $T$-matrix as
\begin{equation}
  \hat{G}=\hat{G}_0+\hat{G}_0\hat{T}\hat{G}_0,
\end{equation}
and a comparison of this definition with \eqref{eq:disordered-dyson-1}
just gives
\begin{equation}
  \label{eq:T-matrix}
  \hat{T}=\hat{T}(E)=\hat{V}+\hat{V}\hat{G}_0\hat{V}+
  \hat{V}\hat{G}_0\hat{V}\hat{G}_0\hat{V}+\cdots.
\end{equation}
This $T$-matrix is an infinite series and contains all the scattering
of $\hat{G}_0$ caused by the off diagonal disorders $\hat{V}$.

Since there is no disorder in $\hat{G}_0$, the average of $\hat{G}$
can be expressed in terms of $T$-matrix as
\begin{equation}
  \label{eq:average-G-T}
  \average{\hat{G}}=\hat{G}_0+\hat{G}_0\average{\hat{T}}\hat{G}_0.
\end{equation}
We can also define a matrix of self-energy for the average Green's
function in the usual way as
\begin{equation}
  \hat{\Sigma}(E)=\hat{G}_0^{-1}-\average{\hat{G}}^{-1},
\end{equation}
or, in Dyson equation form, 
\begin{equation}
  \label{eq:average-G-Sigma}
  \average{\hat{G}}=\hat{G}_0+\hat{G}_0\hat{\Sigma}\average{\hat{G}}=
  \hat{G}_0+\average{\hat{G}}\hat{\Sigma}\hat{G}_0.
\end{equation}
A comparison of \eqref{eq:average-G-T} and \eqref{eq:average-G-Sigma}
immediately gives the relation between the self-energy and $T$-matrix
as
\begin{equation}
  \label{eq:average-sigma-T}
  \hat{\Sigma}=\average{\hat{T}}(1+\hat{G}_0\average{\hat{T}})^{-1}=
  (1+\average{\hat{T}}\hat{G}_0)^{-1}\average{\hat{T}}.
\end{equation}
This expression states that the self-energy of the average Green's
function can be determined by the average $T$-matrix
$\average{\hat{T}}$. However, single impurity scattering problem can
be solved exactly, so here we shall introduce the single site
$t$-matrix.

To introduce $t$-matrix, we first split the random potential matrix
$\hat{V}$ into
\begin{equation}
  \hat{V}=\sum_i\hat{V}_i,
\end{equation}
where $\hat{V}$ is a diagonal matrix while $\hat{V}_i$ is also a
diagonal matrix but only at site $i$ it has value $V_i$. Explicitly,
\begin{equation}
  \hat{V}=\begin{pmatrix}
    V_1&0&0&0&\cdots\\
    0&V_2&0&0&\cdots\\
    \vdots&0&\ddots&0&\vdots\\
    \cdots&0&0&V_{n-1}&0\\
    \cdots&0&0&0&V_{n}\\
  \end{pmatrix},\qquad
  \hat{V}_i=\begin{pmatrix}
    0&0&0&0&\cdots\\
    0&\ddots&0&0&\cdots\\
    \vdots&0&V_i&0&\vdots\\
    \cdots&0&0&\ddots&0\\
    \cdots&0&0&0&0\\
  \end{pmatrix}.
\end{equation}
The infinite series expression \eqref{eq:T-matrix} can be treated as
the power expansions of
\begin{equation}
  \hat{T}=\hat{V}(1-\hat{G}_0\hat{V})^{-1},
\end{equation}
or we write this expression as
\begin{equation}
  \hat{T}(1-\hat{G}_0\hat{V})=\hat{V}.
\end{equation}
Therefore we obtain a self-consistently formula for $\hat{T}$ as
\begin{equation}
  \hat{T}=(1+\hat{T}\hat{G}_0)\hat{V}.
\end{equation}
Now write $\hat{T}=\sum_{i}\hat{T}_i$, where the matrix $\hat{T}_i$
represents the contribution of site $i$ to $T$-matrix, and we have
\begin{equation}
  \sum_{i}\hat{T}_i=[1+(\sum_{i}\hat{T}_i)\hat{G}_0]\hat{V}=
  \sum_{i}[1+(\sum_{j}\hat{T}_j)\hat{G}_0]\hat{V}_i,
\end{equation}
or, comparing the both sides the expression,
\begin{equation}
  \label{eq:T-matrix-i}
  \hat{T}_i=[1+(\sum_j\hat{T}_j)\hat{G}_0]\hat{V}_i=
  (\hat{V}_i+\hat{T}_i\hat{G}_0\hat{V}_i)+\sum_{j\ne i}\hat{T}_j\hat{G}_0\hat{V}_i.
\end{equation}

Now define the single site $t$-matrix at site $i$ as
\begin{equation}
  \hat{t}_i=\hat{V}_i(1-\hat{G}_0\hat{V}_i)^{-1}=
  \hat{V}_i+\hat{V}_i\hat{G}_0\hat{V}_i+\hat{V}_i\hat{G}_0\hat{V}_i\hat{G}_0\hat{V}_i+\cdots.
\end{equation}
This $\hat{t}_i$ includes all the scattering processes caused by site
$i$. If we are dealing with the single impurity scattering problem
where the system has only one impurity, then this $\hat{t}_i$ is also
the full $T$-matrix. This $\hat{t}_i$ is clear solvable, thus the
single impurity scattering problem is also solvable. Write
\eqref{eq:T-matrix-i} as
\begin{equation}
  \hat{T}_i(1-\hat{G}_0\hat{V}_i)=\hat{V}_i+\sum_{j\ne i}\hat{T}_j\hat{G}_0\hat{V}_i,
\end{equation}
and then we have
\begin{equation}
  \label{eq:T-matrix-Tt}
  \begin{array}{rcl}
    \hat{T}_i&=&\displaystyle\hat{V}_i(1-\hat{G}_0\hat{V}_i)
    +\sum_{j\ne i}\hat{T}_j\hat{G}_0\hat{V}_i(1-\hat{G}_0\hat{V}_i)\\\vbox to 14pt{}
    &=&\displaystyle\hat{t}_i+\sum_{j\ne i}\hat{T}_j\hat{G}_0\hat{t}_i.
  \end{array}
\end{equation}
This is the formal solution of $\hat{T}_i$, and it not only includes
the contribution from the scattering at site $i$ but also but the
scattering effects from other sites. We also obtain an infinite series
expression for $\hat{T}$ as
\begin{equation}
  \label{eq:T-matrix-t}
  \hat{T}=\sum_{i}\hat{T}_i=\sum_i\hat{t}_i+\sum_i\sum_{j\ne i}\hat{t}_j\hat{G}_0\hat{t}_i
  +\sum_i\sum_{j\ne i}\sum_{k\ne i,j}\hat{t}_k\hat{G}_0\hat{t}_j\hat{G}_0\hat{t}_i+\cdots.
\end{equation}
When calculating $\average{\hat{T}}$, infinite correlation terms of
$t$-matrix, such as $\average{\hat{t}_j\hat{G}_0\hat{t}_i}$ and
$\hat{t}_k\hat{G}_0\hat{t}_j\hat{G}_0\hat{t}_i$, are involved. In
principle it is impossible to calculate all these terms, so we need
some approximation. 

The most common approximation is called the \textit{single site
  approximation} which neglect the correlations between different
site. Under such an approximation the average of \eqref{eq:T-matrix-t}
can be written as
\begin{equation}
  \average{\hat{T}}=\sum_i\average{\hat{t}_i}+
  \sum_i\sum_{j\ne i}\average{\hat{t}_j}\hat{G}_0\average{\hat{t}_i}
  +\sum_i\sum_{j\ne i}\sum_{k\ne i,j}\average{\hat{t}_k}
  \hat{G}_0\average{\hat{t}_j}\hat{G}_0\average{\hat{t}_i}+\cdots,
\end{equation}
and the average of \eqref{eq:T-matrix-Tt} can be written as
\begin{equation}
  \begin{array}{rcl}
    \average{\hat{T}_i}&=&\displaystyle\average{\hat{t}_i}+\sum_{j\ne i}
    \average{\hat{T}_j}\hat{G}_0\average{\hat{t}_i}\\\vbox to 14pt{}
    &=&(1+\average{\hat{T}}\hat{G}_0)\average{\hat{t}_i}-
    \average{\hat{T}_i}\hat{G}_0\average{\hat{t}_i},
  \end{array}
\end{equation} 
or, moving the last term to the left side of the equation and then
multiplying the inverse on both sides,
\begin{equation}
  \average{\hat{T}_i}=(1+\average{\hat{T}}\hat{G}_0)
  \average{\hat{t}_i}(1+\hat{G}_0\average{\hat{t}_i})^{-1}.
\end{equation}
According to the above expression, we can also write 
\begin{equation}
  \label{eq:T-matrix-average}
  \average{\hat{T}}=\sum_i\average{\hat{T}_i}=
  (1+\average{\hat{T}}\hat{G}_0)\sum_i\average{\hat{t}_i}
  (1+\hat{G}_0\average{\hat{t}_i})^{-1},
\end{equation}
and substituting in \eqref{eq:average-sigma-T} just gives the formula
for self-energy under single site approximation as
\begin{equation}
  \label{eq:average-sigma-t}
  \hat{\Sigma}=\sum_{i}\average{\hat{t}_i}(1+\hat{G}_0\average{\hat{t}_i})^{-1},
\end{equation}
and the corresponding average Green's function matrix
\begin{equation}
  \average{\hat{G}}=(\hat{G}_0^{-1}-\hat{\Sigma})^{-1}.
\end{equation}

\section{Average $T$-Matrix Approximation for Binary Alloy}
Let us apply the average $T$-matrix approximation to a binary alloy
$A_xB_{1-x}$, where $A$ and $B$ are two types of atoms in the alloy
and $x$ is the concentration of atom $A$. In such a system, the random
potential $V_i$ can only have two values $V_A$ and $V_B$, or
explicitly, $V_i=V_A$ when site $i$ is occupied by atom $A$ while
$V_i=V_B$ when site $i$ is occupied by atom $B$.

It is clear that the only non zero element in $\hat{t}_i$ is
\begin{equation}
    \bra{i}\hat{t}_i\ket{i}=\bra{i}\hat{V}_i
    (1-\hat{G}_0\hat{V}_i)^{-1}\ket{i}
    =\frac{V_i}{1-\bra{i}\hat{G}_0\ket{i}V_i},
\end{equation}
where $V_i$ can be $V_A$ and $V_B$ depending on the atom at site
$i$. Since the ordered Green's function $\bra{i}\hat{G}_0\ket{i}$ has
transitional invariance, it is also the unperturbed Green's averaging
over sites:
\begin{equation}
  \bra{i}\hat{G}_0\ket{i}=\frac{1}{N}\sum_i\bra{i}\hat{G}_0\ket{i},
\end{equation}
where $N$ is the number of sites.  Now denote this average by $F(E)$
that $F(E)=\frac{1}{N}\sum_i\bra{i}\hat{G}_0(E)\ket{i}$. If
$\hat{H}_0$ is written in $\bm{k}$-representation that
$\hat{H}_0=\sum_{\bm{k}}\hat{c}_{\bm{k}}^\dagger\hat{c}_{\bm{k}}$,
then we have
\begin{equation}
  F(E)=\frac{1}{N}\sum_{\bm{k}}(E-\varepsilon_{\bm{k}})^{-1}.
\end{equation}
Since the value of $V_i$ can only be $V_A$ and $V_B$, thus the mean
value of $\bra{i}\hat{t}_i\ket{i}$ is just
\begin{equation}
  \overline{\bra{i}\hat{t}_i\ket{i}}=xt_A+(1-x)t_B,
\end{equation}
where
\begin{equation}
  t_A=\frac{V_A}{1-FV_A},\quad t_B=\frac{V_B}{1-FV_B}.
\end{equation}
According to \eqref{eq:average-sigma-t}, the self-energy is a diagonal
matrix and its diagonal elements are all equal to 
\begin{equation}
  \label{eq:average-sigma}
  \Sigma=\frac{[xt_A+(1-x)t_B]}{1+F(E)[xt_A+(1-x)t_B]}.
\end{equation}
Let $\hat{I}$ be the identity matrix, then we can write the
self-energy matrix as
\begin{equation}
  \hat{\Sigma}=\Sigma\hat{I}.
\end{equation}

\subsection*{Defect of the Average $T$-Matrix Approximation}
Using the single site approximation we can obtain the average
self-energy and accordingly the average Green's function. However,
there is a defect in this method that the calculation results depend
on the reference crystal.

To see this, let us first choose atom $B$ as reference crystal, thus
the unperturbed Hamiltonian may be written
\begin{equation}
  \hat{H}_0=\sum_{\bm{k}}(\varepsilon_{\bm{k}}+V_B)\hat{c}_{\bm{k}}^\dagger\hat{c}_{\bm{k}},
\end{equation}
while the perturbation operator is
\begin{equation}
  \hat{H}_1=\sum_{i\in A}(V_A-V_B)\hat{c}_{\bm{k}}^\dagger\hat{c}_{\bm{k}},
\end{equation}
where the summation is over the sites occupied by atom $A$. In this
case the scatterings occur at sites occupied by atom $A$, thus we have
\begin{equation}
  t_A=\frac{(V_A-V_B)}{1-F(E-V_B)(V_A-V_B)},\quad t_B=0,
\end{equation}
where the average Green's function $F$ is
\begin{equation}
  F(E-V_B)=\frac{1}{N}\sum_{\bm{k}}[(E-V_B)-\varepsilon_{\bm{k}}]^{-1}.
\end{equation}
Then according to \eqref{eq:average-sigma} the self-energy with atom
$B$ as reference crystal is
\begin{equation}
  \Sigma_B=\frac{x(V_A-V_B)}{1-(1-x)F(E-V_B)(V_A-V_B)}.
\end{equation}
Similarly, if we choose atom $A$ as reference crystal, the unperturbed
Hamiltonian would become
\begin{equation}
  \hat{H}_0=\sum_{\bm{k}}(\varepsilon_{\bm{k}}+V_A)\hat{c}_{\bm{k}}^\dagger\hat{c}_{\bm{k}},
\end{equation}
and accordingly
\begin{equation}
  t_A=0,\quad t_B=\frac{V_B-V_A}{1-F(E-V_A)(V_B-V_A)}.
\end{equation}
Hence the expression for the self-energy with atom $A$ as reference
crystal is
\begin{equation}
  \Sigma_A=\frac{(1-x)(V_B-V_A)}{1-xF(E-V_A)(V_B-V_A)}.
\end{equation}

From the expression for $\Sigma_B$ and $\Sigma_A$ it is easy to see
that they contradict each other, and
\begin{equation}
  \Sigma_A\sim 1-x,\quad \Sigma_B\sim x.
\end{equation}
Therefore $\Sigma_B\sim x$ only applicable to small $x$ region
($x\ll1$) while $\Sigma_A\sim 1-x$ is only applicable for $x\to1$
region and neither of these two reference crystal alone can describe
the properties of the system for all $0\le x\le1$. In practice, we
need to choose a proper reference crystal first to apply the average
$T$-matrix approximation.

\subsection*{Virtual Crystal Approximation}
Sometimes we can choose a virtual crystal as the reference crystal,
which means that we define a new potential as
\begin{equation}
  \overline{V}=\average{V_i}=xV_A+(1-x)V_B,
\end{equation}
and choose a virtual lattice with this potential as the reference
crystal. This is called the \textit{Virtual Crystal
  Approximation}. Hence the unperturbed Hamiltonian become
\begin{equation}
  \hat{H}_0=\sum_{\bm{k}}(\varepsilon_{\bm{k}}+\overline{V})
  \hat{c}_{\bm{k}}^\dagger\hat{c}_{\bm{k}},
\end{equation}
and accordingly we have
\begin{equation}
  t_A=\frac{V_A-\overline{V}}{1-F(E-\overline{V})(V_A-\overline{V})},\quad
  t_B=\frac{V_B-\overline{V}}{1-F(E-\overline{V})(V_B-\overline{V})}.
\end{equation}
According to the above expressions we can write
\begin{equation}
  \begin{array}{rcl}
    xt_A+(1-x)t_B&=&\displaystyle x(1-x)(V_A-V_B)\left[\frac{1}{1-F(E-\overline{V})(V_A-V_B)}
      -\frac{1}{1+F(E+\overline{V})(V_A-V_B)}\right]\\\vbox to 18pt{}
    &\approx&x(1-x)F(E-\overline{V})(V_A-V_B)^2,
  \end{array}
\end{equation}
and retaining the self-energy with virtual crystal approximation
$\Sigma_{\rm VCA}$ up to second order of powers of $V_A-V_B$ gives
\begin{equation}
  \Sigma_{\rm VCA}(E)\approx x(1-x)(V_A-V_B)^2F(E-\overline{V}).
\end{equation}
Since the Green's function defined in average $T$-matrix approximation
is the retarded Green's function, we need to add an imaginary
infinitesimal $i0$ to the energy $E$, so that
\begin{equation}
  \Sigma_{\rm VCA}(E+i0)\approx x(1-x)(V_A-V_B)^2 F(E+i0-\overline{V}),
\end{equation}
and imaginary part of the self-energy is
\begin{equation}
  \Im\,\Sigma_{\rm VCA}(E+i0)\approx x(1-x)(V_A-V_B)^2\Im\, F(E+i0-\overline{V}).
\end{equation}
The imaginary part of the retarded Green's function $F$ is
proportional to the density of states and is negative, thus
\begin{equation}
  \Im\,\Sigma_{\rm VCA}(E+i0)\sim -x(1-x)(V_A-V_B)^2.
\end{equation}
As we know from Drude theory, 
\begin{equation}
  \Im\,\Sigma_{\rm VCA}(E+i0)=-\frac{\hbar}{2\tau(E)},
\end{equation}
where $\tau$ is the relaxation time. Since the electrical conductivity
$\sigma$ is proportional to the relaxation time so that
\begin{equation}
  \sigma\propto -\frac{1}{\Im\,\Sigma_{\rm VCA}},
\end{equation}
or
\begin{equation}
  \sigma\propto \frac{1}{x(1-x)}.
\end{equation}
Therefore the resistivity $1/\sigma$ is proportional to $x(1-x)$, and
this is just the \textit{Nordheim's rule}. 

\section{Coherent Potential Approximation}

According to the discussion in last section we find that the
calculation of the average $T$-matrix approximation is heavily
dependent on the choice of reference crystal, this makes it hard to
control the results. The \textit{coherent potential approximation}
avoids such problems by using self consistent procedures to produce an
effective lattice. The basic ideas of the coherent potential
approximation was proposed by Paul Soven and David Taylor in
1967. However, the essential idea can be even dated back to James
Maxwell in his effective medium approximation.

Note that although the Green's function $\hat{G}$ is disordered, the
average Green's function $\average{\hat{G}}$ is ordered and
transitional invariant. Thus we try to use an uniform effective medium
to replace all the scattering effects caused by disorders. Let
$\Sigma(E)$ denote the uniform potential of the effective medium. It
should be noted that this uniform potential of the virtual effective
medium is energy dependent. Thus the effective Hamiltonian may be
written as
\begin{equation}
  \hat{H}_e=\sum_{\bm{k}}[\varepsilon_{\bm{k}}+\Sigma(E)]
  \hat{c}^\dagger_{\bm{k}}\hat{c}_{\bm{k}},
\end{equation}
and accordingly the average effective Green's function is
\begin{equation}
  \label{eq:effective-Green-function}
  G_e(E)\equiv\frac{1}{N}\sum_{\bm{k}}[E-\varepsilon_{\bm{k}}-\Sigma(E)]^{-1}.
\end{equation}
It is seen that the effective potential $\Sigma(E)$ acts as an energy
dependent self-energy. Such a self-energy neglects all $\bm{k}$
dependence and thus is a local self-energy. This is the approximation
made in this method.

Now the question becomes how to choose the effective medium so that
the effective Green's function $G_e$ can represent the average
Green's function 
\begin{equation}
  \average{G}=\frac{1}{N}\Tr\,\hat{G}.
\end{equation}
According to \eqref{eq:T-matrix-average} the $T$-matrix for the
effective Hamiltonian can be written as
\begin{equation}
  \average{\hat{T}}=\sum_i\average{\hat{T}_i}=
  (1+\average{\hat{T}}\hat{G}_e)\sum_i\average{\hat{t}_i}
  (1+\hat{G}_e\average{\hat{t}_i})^{-1}.
\end{equation}
In principle, when this average $T$-matrix equal to zero, all the
scattering effects are absorbed into the effective self-energy
$\Sigma(E)$. In this case we can use the average effective Green's
function $G_e$ to replace $\average{G}$. To make $\average{T}=0$ it is
sufficient to require the average $t$-matrix equal to zero that
\begin{equation}
  \label{eq:CPA-condition}
  \average{t_i}=\left\langle\frac{V_i-\Sigma}{1-(V_i-\Sigma)G_e}\right\rangle=0.
\end{equation}
This condition can be expressed in a more useful form. Let us write it as
\begin{equation}
  \left\langle\frac{1}{G_e}\left[1-\frac{1}{1-(V_i-\Sigma)G_e}\right]\right\rangle=0,
\end{equation}
which means 
\begin{equation}
  \label{eq:CPA-condition-1}
  \left\langle\frac{1}{1-(V_i-\Sigma)G_e}\right\rangle=1.
\end{equation}
From \eqref{eq:CPA-condition-1} it can be seen that the condition
\eqref{eq:CPA-condition} can be rewritten as
\begin{equation}
  \left\langle\frac{V_i}{1-(V_i-\Sigma)G_e}\right\rangle=
  \left\langle\frac{\Sigma}{1-(V_i-\Sigma)G_e}\right\rangle,
\end{equation}
and since $\Sigma$ is a constant for specific $E$, we finally have
\begin{equation}
  \label{eq:CPA-condition-2}
  \left\langle\frac{V_i}{1-(V_i-\Sigma)G_e}\right\rangle=\Sigma.
\end{equation}
This expression is just the self consistent condition which is needed
when determining the effective potential (self-energy for effective
Green's function) $\Sigma$.

Here we list the basic procedures of the coherent potential
approximation:
\begin{enumerate}[1.]\setlength{\itemsep}{0pt}
\item choose an initial self-energy $\Sigma(E)$ for specific $E$;
\item use \eqref{eq:effective-Green-function} to calculate the
  average effective Green's function $G_e$;

  It should be noted here that \eqref{eq:effective-Green-function} is
  not practical in real calculation, and usually another form of this
  formula is used that
  \begin{equation}
    \label{eq:effective-Green-function-DOS}
    G_e=\int\frac{\rho_0(\varepsilon)}{E-\varepsilon-\Sigma}d\varepsilon,
  \end{equation}
  where $\rho_0(\varepsilon)$ is the density of states of unperturbed
  Hamiltonian. The above formula is essentially equivalent to
  \eqref{eq:effective-Green-function} but it is more practical since
  the density of states can be calculated via other methods, for
  instance, for the cubic lattice there is analytic formulas for its
  density of states.
\item use \eqref{eq:CPA-condition-2} to calculate a new self-energy
  $\Sigma(E)$;
\item compare the new self-energy with the old self-energy, if they
  are close enough then finish the self consistent procedures,
  otherwise go to step 2 again.
\end{enumerate}

The self-energy is determined self consistently, thus the difficulty
of choosing a proper reference lattice in the average $T$-matrix
approximation is avoided. The self-energy $\Sigma(E)$ obtained in this
way is usually a complex number, and according the discussion in Drude
theory its imaginary part is related to the relaxation time. Therefore
we can use Drude formula to calculate the conductivity. We can also
use this self-energy to apply Kubo-Greenwood formula with the Green's
function in $\bm{k}$-representation
\begin{equation}
  G_e(E,\bm{k})=\frac{1}{E-\varepsilon_{\bm{k}}-\Sigma(E)}.
\end{equation}

\subsection*{Coherent Potential Approximation for Binary Alloy}
Here we shall use a cubic binary alloy $A_xB_{1-x}$ as an example. The
Hamiltonian is 
\begin{equation}
  \hat{H}=W\!\!\sum_{\average{ij}}\hat{c}_i^\dagger\hat{c}_j+\sum_iV_i\hat{c}_i^\dagger\hat{c}_i,
\end{equation}
where the symbol $\average{}$ indicates that summation is over the
nearest neighbors and $W$ is the hopping energy among nearest
neighbors. And for potential $V_i$ we have
\begin{equation}
  V_i=\left\{\begin{array}{ll}
      V_A & i\in A;\\
      V_B & i\in B,\\
    \end{array}\right.
\end{equation}
where $i\in A$ means that site $i$ is occupied by atom $A$ and similar
rule is for atom $B$.

In a binary alloy $A_xB_{1-x}$ the self consistent condition
\eqref{eq:CPA-condition-2} becomes simply
\begin{equation}
  \Sigma=\frac{xV_A}{1-(V_A-\Sigma)G_e}+\frac{(1-x)V_B}{1-(V_B-\Sigma)G_e}.
\end{equation}
To apply formula \eqref{eq:effective-Green-function-DOS} we need the
density of states of cubic lattice. 

{\Huge \textcolor{red}{here should be numerical results}}


\section{Anderson Localization}
A lattice with perfect periodicity is transitional invariant, and the
wave function of an electron in it can be represented by Bloch
functions in $\bm{k}$-representation that
\begin{equation}
  \psi_{\bm{k}}(\bm{r})=u_{\bm{k}}(\bm{r})e^{i\bm{k}\cdot\bm{r}},
\end{equation}
where $u_{\bm{k}}$ is a periodic function. Such a wave function is
like the plane wave and extended in the whole space, thus the states
represented by Bloch waves are called \textit{extended states}. On the
contrary, there also exist some states that the electron is not
extended, i.e., localized in some regions. For example, the wave
function of an electron in high barrier is exponential decay like
\begin{equation}
  \psi(\bm{r})=\sim e^{-|\bm{r}|/l_0},
\end{equation}
where $l_0$ is a positive constant. Clearly that the electron states
represented by such kind of wave functions are not extended. And these
states, which are different from extended states, are called
\textit{localized states}. The electrons in localized states have
little contributions to electrical conductivity.

The classical theories, such as Boltzmann equation, treat the
disorders in the lattice as scattering sources and formulate the
impurity scattering theory. The $T$-matrix formalism and coherent
potential approximation basically do the same thing. In such
theoretical frame, the disorders scatter Bloch electron and decrease
the relaxation time, and thus decrease the electrical
conductivity. The conductivity in this frame can never reach zero
unless the scattering strength is infinite large. However, Philip
Anderson pointed out that even the scattering strength is finite, the
randomness of the system may also cause localization states in 1957.

\subsection*{Anderson Disorder Model}
Anderson use the tight-binding Hamiltonian with disorders to describe
a diagonal disordered system, which is now called \textit{Anderson
  disorder model}. The Hamiltonian for Anderson disordered model has
the same form with \eqref{eq:disordered-hamiltonian} that
\begin{equation}
  \hat{H}=\sum_{ij}W_{ij}\hat{c}^\dag_i\hat{c}_j
  +\sum_{i}V_i\hat{c}^\dagger_i\hat{c}_i.
\end{equation}
Note that the off diagonal elements $W_{ij}$ of the Hamiltonian is
ordered, the disorders only exist in diagonal elements. If only the
nearest neighbors hopping are under consideration, the Hamiltonian
reduces to
\begin{equation}
  \hat{H}=W\!\!\sum_{\average{ij}}\hat{c}^\dag_i\hat{c}_j
  +\sum_{i}V_i\hat{c}^\dagger_i\hat{c}_i,
\end{equation}
where $W$ is some constant and the symbol $\average{}$ indicates that
the summation is over the nearest neighbors.

And the disordered potential $V_i$ is assumed to be distributed
uniformly in an energy range with width $\Gamma$, that the
distribution probability density is given by
\begin{equation}
  \label{eq:anderson-distribution}
  P(V_i)=\left\{\begin{array}{ll}
      \displaystyle\frac{1}{\Gamma}, & 
      \displaystyle|V_i|\le\Gamma/2;\\\vbox to 18pt{}
      0, & \displaystyle|V_i|\ge\Gamma/2.\\
    \end{array}\right.
\end{equation}
The disorders formulated in this way is short ranged and is
characterized by an important parameter $\Gamma$ which represents the
degree of randomness.

\subsection*{Anderson Localization}
To investigate localization phenomena in the Anderson disordered model
we need first find the criteria of localized states. Recall that the
Green's function $G_{ij}(t)$ gives the amplitude of finding the
electron at site $i$ at time $t$ with the initial position at site $j$
at time $0$, and its square modulus $|G_{ij}(t)|^2$ gives the
corresponding probability of finding the electron at site $i$ at time
$t$. Therefore $|G_{ii}(t)|^2$ gives the probability that an electron
returns to its original site $i$ after time $t$. If $t$ is sufficient
long and $G_{ii}(t)$ is zero, then the electron would not return to
the origin after a long time, which means that the electron leaves the
origin and propagates in the whole system, thus it is in extended
states. On the other hand, the probability of electron that not return
to original site in future counting from an arbitrary time $t_0>0$ is
\begin{equation}
  \prod_{t_i=t_0}^\infty[1-|G_{ii}(t_i)|^2],
\end{equation}
where this product is understand to take the limit over all the
infinitesimal intervals $\delta t$ between $t_0$ and $t\to\infty$.  It
is clear that this quantity tends to zero if $G_{ii}(t)$ is not zero
for large $t$, which means that electron would certainly return to the
origin at some time in the distant future. And thus the electron is in
localized states.

The equation of motion for the Green's function in energy
representation is same as \eqref{eq:equation-motion} that
\begin{equation}
  \label{eq:equation-motion-anderson}
  EG_{ij}(E)=V_{i}G_{ij}(E)+\sum_kW_{ik}G_{kj}+\delta_{ij},
\end{equation}
or
\begin{equation}
  (E-V_i)G_{ij}(E)=\sum_kW_{ik}G_{kj}+\delta_{ij}.
\end{equation}
Now define a quantity
\begin{equation}
  g_i=\frac{1}{E-V_i},
\end{equation}
then \eqref{eq:equation-motion-anderson} becomes
\begin{equation}
  \begin{array}{rcl}
    G_{ij}(E)&=&\displaystyle g_i\delta_{ij}+\sum_kg_iW_{ik}G_{kj}\\\vbox to 14pt{}
    &=&\displaystyle g_i\delta_{ij}+\sum_{kl}g_i\delta_{ik}W_{kl}G_{lj}.
  \end{array}
\end{equation}
This expression is in Dyson equation form and it is clear that
$g_i\delta_{ij}$ acts the unperturbed Green's function. And the
equation for diagonal elements of the Green's function $G_{ii}$ is
then
\begin{equation}
  G_{ii}(E)=\displaystyle g_i+\sum_{k}g_iW_{ik}G_{ki}.
\end{equation}
The right side of this equation contains off diagonal elements of the
Green's function, now pick the diagonal elements out and do
iteratively substitution that (note that the diagonal terms $W_{ii}=0$
according to definition)
\begin{equation}
  \begin{array}{rcl}
    G_{ii}&=&\displaystyle g_i+g_iW_{ii}G_{ii}+\sum_{k\ne i}g_iW_{ik}G_{ki}\\\vbox to 14pt{}
    &=&\displaystyle g_i+\sum_{k\ne i}g_iW_{ik}g_kW_{ki}G_{ii}+
    \sum_{k\ne i}\sum_{l\ne k,i}g_iW_{ik}g_kW_{kl}G_{li}+\cdots.\\\vbox to 14pt{}
  \end{array}
\end{equation}
Then we obtain the Dyson equation for diagonal Green's function
$G_{ii}$ in term of self-energy that
\begin{equation}
  G_{ii}=g_i+g_i\Sigma_iG_{ii},
\end{equation}
where
\begin{equation}
  \label{eq:anderson-self-energy}
  \Sigma_i(E)=\sum_{k\ne i}W_{ik}g_kW_{ki}+\sum_{k\ne i}\sum_{l\ne k,i}W_{ik}g_kW_{kl}g_lW_{li}+\cdots.
\end{equation}
According to the definition of $g_i$, the formal solution for $G_{ii}(E)$ can be written as
\begin{equation}
  G_{ii}(E)=\frac{1}{E-V_i-\Sigma_i(E)},
\end{equation}
or, since the Green's function is essentially retarded Green's function,
\begin{equation}
  G_{ii}(E)=\frac{1}{E+i0-V_i-\Sigma_i(E)}.
\end{equation}
Clearly the value of the self-energy determines the Green's function,
and the Green's function in time representation with energy $E$ is
given by the inverse Fourier transform that
\begin{equation}
  G_{ii}(t)=\int_{-\infty}^\infty G_{ii}(E)e^{-\frac{i}{\hbar}Et}\frac{dE}{2\pi\hbar}=
  \frac{1}{2\pi\hbar}\int_{-\infty}^\infty\frac{e^{-\frac{i}{\hbar}Et}}{E+i0-V_i-\Sigma_i(E)}dE.
\end{equation}
The inverse Fourier transform is an integration along a contour just
above the real axis in the $E$-plane. If the self-energy $\Sigma_i(E)$
has a non-zero imaginary part, then $G_{ii}(t)$ acquires a
exponentially decay factor $e^{\frac{1}{\hbar}\Im,\Sigma_it}$ and
$G_{ii}(t)$ would tend to zero for large $t$. In another words, a
necessary condition for localized states with energy $E$ is that the
imaginary part of corresponding self-energy $\Sigma_i(E)$ vanishes.
The infinite series expression for self-energy $\Sigma_i$ is just
\eqref{eq:anderson-self-energy}. When $E$ is real, this infinite
series must also be real if it has a limit. In other words, a
sufficient condition that the state with energy $E$ should be
localized is that the infinite series \eqref{eq:anderson-self-energy}
converges.

Now we rewrite the expression of self-energy
\eqref{eq:anderson-self-energy}, by writing $W_{ik}$ as a constant
since it is essentially an ordered quantity, as
\begin{equation}
  \label{eq:self-energy-series}
  \begin{array}{rcl}
    \Sigma_i(E)&=&\displaystyle\sum_{k\ne i}Wg_kW+
    \sum_{k\ne i}\sum_{l\ne k,i}Wg_kWg_lW+\cdots\\\vbox to 14pt{}
    &=&\displaystyle W\sum_{L=1}^\infty\sum_{j}T_j^{(L)},\\
\end{array}
\end{equation}
where 
\begin{equation}
  T^{(L)}_j=\frac{W^L}{(E-V_{j_1})\cdots(E-V_{j_L})}=\prod_{i=1}^LWg_{j_i}.
\end{equation}
It should be emphasized that after replacing the matrix $W_{ik}$ by
constant $W$ the summation must be understood that it is over the
``connected'' sites, and the connection between site $i$ and $k$ means
that the original matrix element $W_{ik}$ is not zero. Therefore
$T^{(L)}_j$ represents a path that starts from site $i$, passes $L$
different sites $j_1,j_2,\cdots,j_L$ and then returns to site
$i$. Note that sites $j_1,j_2,\cdots,j_L$ are all different, and such
a path is generally called \textit{self-avoiding walk}. Suppose each
site has $Z$ neighbors, then in the summation for $\Sigma_i$ there are
roughly $Z^L$ terms $T^{(L)}_j$ for each $L$. Now we introduce a
simple method given by John Ziman to estimate this summation.

Now define a quantity $Q$ as the logarithm of $|T^{(L)}_j|$:
\begin{equation}
  Q=\ln|T^{(L)}_j|=\ln\prod_{i=1}^L|Wg_{j_i}|=\sum_{i=1}^L\ln |Wg_{j_i}|.
\end{equation}
Since $g_{j_i}$ are random variables, we use the average value
\begin{equation}
  \average{\ln|Wg_{j_i}|}=\int dV_iP(V_i)\ln\left|\frac{W}{E-V_i}\right|
\end{equation}
to replace $\ln|Wg_{j_i}|$. It is clear that the average value
$\average{\ln|Wg_{j_i}|}$ is statistically site independent, thus we
omit the subscript and write it simply as
$\average{\ln|Wg|}$. Therefore after the average we write
\begin{equation}
  Q= \sum_{i=1}^L\ln|Wg_{j_i}|\approx L\average{\ln|Wg|},
\end{equation}
and
\begin{equation}
  |T^{(L)}_j|=e^Q\approx\exp\left[L\average{\ln|Wg|}\right].
\end{equation}
Because in the summation for a specific $L$ there are $Z^L$ terms
which have the same form with $T^{(L)}_j$, then we have
\begin{equation}
  \sum_j|T^{(L)}_j|\approx\left[Z\exp(\average{\ln|Wg|})\right]^L.
\end{equation}
There is always an inequality for the series that
\begin{equation}
  \sum_jT^{(L)}_j\le \sum_j|T^{(L)}_j|,
\end{equation}
so if the geometric series 
\begin{equation}
  W\sum_{L=1}^\infty\left[Z\exp(\average{\ln|Wg|})\right]^L
\end{equation}
converges then the original series \eqref{eq:self-energy-series}
converges.

The convergence condition for geometric series is 
\begin{equation}
  Z\exp(\average{\ln|Wg|})<1,
\end{equation}
and this condition can also be used as the localization condition.
Substituting the distribution function
\eqref{eq:anderson-distribution} we find
\begin{equation}
  \begin{array}{rcl}
    \average{\ln|Wg|}&=&\displaystyle\frac{1}{\Gamma}\int_{-\Gamma/2}^{\Gamma/2}\ln
    \left|\frac{W}{E-V}\right|dV\\\vbox to 18pt{}
    &=&\displaystyle 1-\frac{1}{2}\left[\left(1+2\frac{E}{\Gamma}\right)\ln
      \left|\frac{\Gamma}{2W}+\frac{E}{W}\right|+
      \left(1-2\frac{E}{\Gamma}\right)\ln
      \left|\frac{\Gamma}{2W}-\frac{E}{W}\right|
    \right].
\end{array}
\end{equation}
For $E=0$, we have
\begin{equation}
  \average{\ln|Wg|}=1-\ln\left|\frac{\Gamma}{2W}\right|,
\end{equation}
and the localization condition for $E=0$ is correspondingly
\begin{equation}
  \label{eq:anderson-localization-condition}
  \frac{\Gamma}{2Z|W|}>e.
\end{equation}
Note that the average value $\average{\ln|Wg|}$ reaches its maximum
when $E=0$. Therefore if the localization condition is satisfied for
$E=0$ then all other states with $|E|>0$ also satisfy the convergence
condition $Z\exp(\average{\ln|Wg|})<1$, which means that all states
are localized states. The expression
\eqref{eq:anderson-localization-condition} is also called the
\textit{Anderson localization condition}.



\chapter{Small Polaron}
\section{Holstein Model}
The Hamiltonian of Holstein Model is
\begin{equation}
  H=-\sum_{\langle{i,j}\rangle}t_{ij}c_i^\dag c_j+g\sum_ic_i^\dag c_i(a_i+a_i^\dag)
  +\omega_0\sum_ia_i^\dag a_i,
  \label{Holstein}
\end{equation}
where $c_i^\dag$ ($c_i$) is creation (annihilation) operator for electron, and
$a_i^\dag$ ($a_i$) is creation (annihilation) operator for phonon.

The model possesses two independent control parameters:
\begin{equation}
  \lambda =g^2/\omega_0t,
\end{equation}
\begin{equation}
  \gamma=\omega_0/t.
\end{equation}
A third parameter can be conveniently introduced as a combination of
the above ones:
\begin{equation}
  \alpha=\lambda/\gamma=g/\omega_0.
\end{equation}

It is worth defining the following regimes and limits, which are relevant 
to the Holstein model:

\begin{enumerate}[(i)]
\item weak (strong) couplings $\lambda<1$ ($\lambda>1$);
\item small (large) phonon frequency $\gamma<1$ ($\gamma>1$);
\item multiphonon regime $\alpha^2>1$;
\item adiabatic limit $\omega_0=0$, finite $\lambda$.
\end{enumerate}
%%----------------------------------------------------------------------

\section{Weak Coupling Limit}
Consider zero density ($n=0$) and zero temperature ($T=0$) limits,
Green's function for a single electron can be defined as
\begin{equation}
  G_{ij}(t)=-i\bra{0}Tc_i(t)c_j^\dag(0)\ket{0},
\end{equation}
where $\ket{0}$ is the vacuum for phonons and electrons. There is only
one possible ordering ($t>0$), so the function is purely retarded.

Let $g\sum_ic_i^\dag c_i(a_i+a_i^\dag)$ acts as perturbation, we have
that
\begin{equation}
  G_{ij}(t)=-i\bra{0}Tc_i(t)c_j(0)^\dag S\ket{0},
\end{equation}
where
\begin{equation}
  S=Te^{-i\int dt [g\sum_ic_i^\dag c_i(a_i+a_i^\dag)]}\quad.
\end{equation}
The expansion of $S$ to second order of $g$ gives
\begin{equation}
  \begin{array}{rcl}
  G_{ij}(t)&=&-i\bra{0}Tc_i(t)c_j^\dag\ket{0}\\\vbox to 18pt{}
  & &-i\displaystyle\frac{g^2}{2}\int dt'dt''\sum_{kl}
  \bra{0}Tc_i(t)c_j^\dag c_k^\dag(t')c_k(t')c_l^\dag(t'')c_l(t'')
  [a_k(t')a_l^\dag(t'')+a_k^\dag(t')a_l(t'')]\ket{0},\\
  \end{array}
\end{equation}
apply Wick's theorem and recall that ($D$ is the Green's function for phonon)
\begin{equation}
  \begin{array}{l}
    \bra{0}a_k^\dag(t')a_l(t'')\ket{0}=0,\\\vbox to 18pt{}
    \bra{0}a_k(t')a_l^\dag(t'')\ket{0}=D_{kl}(t'-t'')=\delta_{kl}D_{kk}(t'-t''),\\
  \end{array}
\end{equation}
we can obtain that
\begin{equation}
  G_{ij}(t)=G^{(0)}_{ij}(t)+ig^2\sum_k\int dt'dt''G^{(0)}_{ik}(t-t')
  G^{(0)}_{kk}(t'-t'')D_{kk}(t'-t'')G_{kj}(t''),
\end{equation}
in frequency space, (note that $D_{kk}(t'-t'')=-ie^{-i\omega_0(t'-t'')}$)
\begin{equation}
  G_{ij}(\omega)=G^{(0)}_{ij}(\omega)+
  g^2\sum_{k}G^{(0)}_{ik}(\omega)G_{kk}^{(0)}(\omega-\omega_0)G^{0}_{kj}(\omega).
\end{equation}
Compare with the Dyson equation
\begin{equation}
  G_{ij}=G^{(0)}_{ij}+\sum_{kl}G^{(0)}_{ik}\Sigma_{kl}G_{lj}
  =G^{(0)}_{ij}+\sum_{kl}G^{(0)}_{ik}\Sigma_{kl}G^{(0)}_{lj}+\cdots\quad,
\end{equation}
it is clear to see that second order perturbation gives a local
($k$-independent) self energy:
\begin{equation}
  \Sigma_2(\omega)=g^2G^{(0)}(\omega-\omega_0).
\end{equation}

The electron effective mass, in the case of a local self-energy, is
easily calculated via
\begin{equation}
  \frac{m^*}{m}=\left.\frac{d(\omega-\Re\Sigma(\omega))}{d\omega}\right|_{E_0}
  =1-\left.\frac{d\Re\Sigma(\omega)}{d\omega}\right|_{E_0},
\end{equation}
where $E_0$ is the ground-state energy.
%%----------------------------------------------------------------------------

\section{Atomic Limit (Zero Temperature)}
The atomic limit is defined as the zero hopping case ($t=0$).
In this case, Hamiltonian \eqref{Holstein} can be diagonalized by 
the unitary Lang-Firsov transformation
\begin{equation}
  U=e^{-S},\quad S=-\alpha\sum_{i}c_i^\dag c_i(a_i-a_i^\dag).
\end{equation}
With the aid of Baker-Campbell-Hausdorff formula we have
\begin{equation}
  \begin{array}{l}
    \bar{c}_i=e^Sc_ie^{-S}=c_iX_i,\quad X_i=e^{\alpha(a_i-a_i^\dag)};\\\vbox to 18pt{}
    \bar{c}_i^\dag=e^Sc_i^\dag e^{-S}=c_i^\dag X_i^\dag, \quad
    X_i^\dag=e^{\alpha(a_i^\dag-a_i)};\\\vbox to 18pt{}
    \bar{a}_i=e^Sa_ie^{-S}=a_i-\alpha c_i^\dag c_i;\\\vbox to 18pt{}
    \bar{a}_i^\dag=e^Sa_i^\dag e^{-S}=a_i^\dag-\alpha c_i^\dag c_i;\\\vbox to 18pt{}
    \bar{H}=e^SHe^{-S}=-\frac{g^2}{\omega_0}\sum_ic_i^\dag c_i+
    \omega_0\sum_ia_i^\dag a_i.\\
  \end{array}
\end{equation}
After the transformation, we can see that the ground energy is
$\varepsilon_p=-g^2/\omega_0$, the excited state energy is 
$\varepsilon_p+n\omega_0$.

The static electron-displacement correlation function is defined as
$C_0=\langle n_i(a_i+a_i^\dag)\rangle$, apply Lang-Firsov
transformation it reads
\begin{equation}
  C_0=\langle n_i(a_i+a_i^\dag)\rangle-2\alpha\langle n_i\rangle
  =-2\alpha\langle n_i\rangle,
\end{equation}
at the ground state $n_i=1$, thus $C_0=-2\alpha$. Meanwhile, 
\begin{equation}
  \langle e^Sa^\dag ae^{-S}\rangle=\langle a^\dag a\rangle+
  \alpha^2\langle c^\dag c\rangle=\alpha^2.
\end{equation}

The electron Green's function can also be calculated after the
Lang-Firsov transformation\footnote{Mahan's Many-Particle Physics, page 221}:
\begin{equation}
  \begin{array}{rcl}
  G(t)&=&-i\bra{0}c(t)c^\dag\ket{0}\\\vbox to 18pt{}
  &=&-i\bra{0}cXe^{-i\bar{H}t}c^\dag X^\dag\ket{0}\\\vbox to 18pt{}
  &=&-i\displaystyle\sum_{mn}\bra{0}cX\ket{m}\bra{m}e^{-i\bar{H}t}\ket{n}\bra{n}
  c^\dag X^\dag\ket{0},\\
  \end{array}
\end{equation}
where $\ket{m}$ is the phonon state corresponding to $m$ phonons.

Using the Feynman result ($e^{A+B}=e^Ae^Be^{-\frac{1}{2}[A,B]}$), we have that
\begin{equation}
  X^\dag=e^{-\alpha^2/2}e^{\alpha a^\dag}e^{-\alpha a},\quad
  X=e^{-\alpha^2/2}e^{-\alpha a^\dag}e^{\alpha a},
\end{equation}
accordingly,
\begin{equation}
  \begin{array}{l}
  \displaystyle\bra{m}X^\dag\ket{0}=e^{-\alpha^2/2}\bra{m}e^{\alpha a^\dag}\ket{0}
  =e^{-\alpha^2/2}\sum_n\bra{m}\frac{\alpha^n}{\sqrt{n!}}\ket{n}
  =e^{-\alpha^2/2}\frac{\alpha^m}{\sqrt{m!}},\\\vbox to 18pt{}
  \displaystyle\bra{0}X\ket{m}=e^{-\alpha^2/2}\frac{\alpha^m}{\sqrt{m!}}.
  \end{array}
\end{equation}
Finally the electron Green's function is
\begin{equation}
  G(\omega)=\sum_{n}^\infty \frac{\alpha^{2n}e^{-\alpha^2}}{n!}
  \frac{1}{\omega-n\omega_0-\varepsilon_p}.
\end{equation}

Let us now consider the action of the hopping. After the Lang-Firsov
transformation, the hopping term becomes
\begin{equation}
  t_{ij}c_i^\dag c_j\quad\to\quad t_{ij}X_i^\dag X_jc_i^\dag c_j,
\end{equation}
consider Holstein approximation, which neglect phonon emission and
absorption during the hopping process, we have
\begin{equation}
  t_{ij}\bra{0}X_i^\dag X_j\ket{0}=t_{ij}\bra{0}X^\dag\ket{0}\bra{0}X\ket{0}
  =t_{ij}e^{-\alpha^2}.
\end{equation}
%%---------------------------------------------------------------------------

\section{Atomic Limit (Finite Temperature)}
The Lang-Firsov transformation is the same as zero temperature
case. Here we need to calculate $\bra{n}X^\dag\ket{n}$. We have that
\begin{equation}
  \begin{array}{rcl}
  e^{-\alpha a}\ket{n}&=&\displaystyle\sum_{m=0}^\infty
  \frac{(-\alpha)^m}{m!}a^m\ket{n}\\\vbox to 20pt{}
  &=&\displaystyle\sum_{m=0}^n\frac{(-\alpha)^m}{m!}\left[
  \frac{n!}{(n-m)!}\right]^{\frac{1}{2}}\ket{n-m},
  \end{array}
\end{equation}
and
\begin{equation}
  \bra{n}e^{\alpha a^\dag}=\sum_{m=0}^n\frac{\alpha^m}{m!}\left[
  \frac{n!}{(n-m)!}\right]^{\frac{1}{2}}\bra{n-m},
\end{equation}
therefore
\begin{equation}
  \bra{n}e^{\alpha a^\dag}e^{-\alpha a}\ket{n}=
  \sum_{m=0}^n\frac{(-\alpha^2)^m}{m!}\frac{n!}{m!(n-m)!}=L_n(\alpha^2),
\end{equation}
where $L_n(x)$ is Laguerre polynomial. Thus
\begin{equation}
  \bra{n}X^\dag\ket{n}=\bra{n}X\ket{n}=e^{-\alpha^2/2}L_n(\alpha^2).
\end{equation}

At finite temperature, the assumption is that we only average on
phonon according to temperature. (``cold'' electron in a thermalized
phonon bath). So at finite temperature the effective hopping amplitude is
\begin{equation}
  \begin{array}{rcl}
    &&\displaystyle t_{ij}(1-e^{-\beta\omega_0})^2\sum_{mn}e^{-\beta m\omega_0}
    \bra{m}X_i^\dag\ket{m}e^{-\beta n\omega_0}\bra{n}X_j\ket{n}
    \\\vbox to 20pt{}
    &=&\displaystyle t_{ij}e^{-\alpha^2}\left[(1-e^{-\beta\omega_0})
      \sum_{n=0}^\infty e^{-n\beta\omega_0}L_n(\alpha^2)\right]^2.\\
  \end{array}
\end{equation}
Recall that the generating function of Laguerre polynomials:
\begin{equation}
  \frac{e^{-xt/(1-t)}}{1-t}=\sum_{n=0}^\infty L_n(x)t^n,
\end{equation}
let $t=e^{-\beta\omega_0}$ and $x=\alpha^2$ we find that the effective
hopping amplitude is
\begin{equation}
t_{ij}e^{-S_T}, \quad  S_T=\alpha^2(1+2\average{n}_T).
\end{equation}

Now let us turn to electron Green's function, now defined as
\begin{equation}
  \begin{array}{rcl}
      G(t)&=&\displaystyle-i(1-e^{-\beta\omega_0})\sum_ne^{-\beta n\omega_0}
      \bra{n}c(t)c^\dag\ket{n}\\\vbox to 18pt{}
      &=&\displaystyle-i(1-e^{-\beta\omega_0})\sum_ne^{-\beta n\omega_0}
      \bra{0}c(t)X(t)c^\dag X^\dag\ket{0}\\\vbox to 18pt{}
      &=&\displaystyle-i(1-e^{-\beta\omega_0})\bra{0}c(t)c^\dag\ket{0}
      \sum_ne^{-\beta n\omega_0}\bra{n}X(t)X^\dag\ket{n}.\\\vbox to 18pt{}
  \end{array}
\end{equation}
According to Heisenberg equation of motion (with Hamiltonian
$\bar{H}$), we have that
\begin{equation}
  \begin{array}{l}
    c(t)=ce^{-i\varepsilon_pt},\quad c^\dag(t)=c^\dag e^{i\varepsilon_pt};\\
    a(t)=ae^{-i\omega_0t},\quad a^\dag(t)=a^\dag e^{i\omega_0t},\\
  \end{array}
\end{equation}
thus
\begin{equation}
  X(t)=e^{-\alpha^2}e^{-\alpha a^\dag e^{i\omega_0t}}e^{\alpha ae^{-i\omega_0t}}=
  e^{-\alpha^2}e^{-\alpha a^\dag(t)}e^{\alpha a(t)}
\end{equation}
and
\begin{equation}
  X(t)X^\dag=e^{-\alpha^2}e^{-\alpha a^\dag(t)}e^{\alpha a(t)}
  e^{\alpha a^\dag}e^{-\alpha a}.
\end{equation}
Now we write $e^{\alpha a(t)}e^{\alpha a^\dag}$ as\footnote{see Mahan's
  Many-Particle Physics, page 222}
\begin{equation}
  e^{\alpha a(t)}e^{\alpha a^\dag}=e^{\alpha a^\dag}[e^{-\alpha
      a^\dag}e^{\alpha a(t)}e^{\alpha a^\dag}],
\end{equation}
using Baker-Campbell-Hausdorff formula we get
\begin{equation}
  e^{-\alpha a^\dag}e^{\alpha a(t)}e^{\alpha a^\dag}=
  e^{\alpha^2e^{-i\omega_0t}}e^{\alpha a(t)}.
\end{equation}
Finally the electron Green's function is arranged into the desired form:
\begin{equation}
  G(t)=-i(1-e^{-\beta\omega_0})e^{-\alpha^2(1-e^{-i\omega_0t})}
  \bra{0}c(t)c^\dag\ket{0}\sum_ne^{-\beta n\omega_0}
  \bra{n}e^{\alpha a^\dag(1-e^{i\omega_0t})}e^{-\alpha a(1-e^{-i\omega_0t})}\ket{n},
\end{equation}
again using Laguerre polynomials we can prove that
\begin{equation}
  (1-e^{-\beta\omega_0})\sum_ne^{-\beta n\omega_0}
  \bra{n}e^{u^*a^\dag}e^{-u a}\ket{n}=e^{-|u|^2/(e^{\beta\omega_0}-1)},
\end{equation}
thus
\begin{equation}
  G(t)=-ie^{-i\varepsilon_pt}
    \exp\Bigl[-\alpha^2[(N+1)(1-e^{-i\omega_0t})+N(1-e^{i\omega_0t})]\Bigr],
\end{equation}
where
\begin{equation}
  N=\frac{1}{e^{\beta\omega_0}-1}.
\end{equation}
Recall the generating function of Bessel functions of complex argument,
\begin{equation}
  e^{z\cos\theta}=\sum_{n=-\infty}^\infty I_n(z)e^{in\theta},
\end{equation}
let [note $(N+1)/N=e^{\beta\omega_0},\sqrt{(N+1)/N}=e^{\beta\omega_0/2}$]
\begin{equation}
  z=2\alpha^2\sqrt{N(N+1)},\quad \theta=\omega_0(t+i\beta/2)
\end{equation}
then (note that $I_{n}=I_{-n}$)
\begin{equation}
  G(t)=-ie^{-(2N+1)\alpha^2}e^{-i\varepsilon_pt}\sum_{n=-\infty}^\infty
  e^{-in\omega_0t}e^{\beta n\omega_0/2}I_n\{2\alpha^2\sqrt{N(N+1)}\},
\end{equation}
in frequency space
\begin{equation}
  G(\omega)=e^{-(2N+1)\alpha^2}\sum_{n=-\infty}^{\infty}
  e^{\beta n\omega_0/2}I_n\{2\alpha^2\sqrt{N(N+1)}\}
  \frac{1}{\omega-n\omega_0-\varepsilon_p}.
\end{equation}

%%---------------------------------------------------------------------

\section{The Impurity Analogy for A Single Electron}
The Hamiltonian for impurity model is
\begin{equation}
  H_{\rm imp}=\sum_k\varepsilon_kc_k^\dag c_k+\sum_kV_k(c_k^\dag d+d^\dag c_k)
  +\omega_0a^\dag a+gd^\dag d(a+a^\dag),
\end{equation}
here $V_k$ and $E_k$ is related to $G_0$ by
\begin{equation}
  G_0^{-1}(\omega)=\omega-\int_{-\infty}^\infty d\varepsilon
  \frac{\Delta(\varepsilon)}{\omega-\varepsilon},
\end{equation}
where
\begin{equation}
  \Delta(\varepsilon)=\sum_kV_k^2\delta(\varepsilon-\varepsilon_k).
\end{equation}

Let us separate the Hamiltonian into two parts $H_0$ and $V$, where
\begin{equation}
  H_0=\sum_k\varepsilon_kc_k^\dag c_k+\sum_kV_k(c_k^\dag d+d^\dag c_k)
  +\omega_0a^\dag a,\quad V=gd^\dag d(a+a^\dag).
\end{equation}

%%----------------------------------------------------------------------
\subsection{The Zero Temperature Formalism}
The Green's function for one electron at zero temperature is
\begin{equation}
  G(t)=-i\theta(t)\bra{0}d(t)d^\dag\ket{0},
\end{equation}
after Fourier transformation:
\begin{equation}
  G(\omega)=\bra{0}d\frac{1}{\omega+i0-H}d^\dag\ket{0}.
\end{equation}
  
An operator identity holds:
\begin{equation}
  \frac{1}{\omega-H}=\frac{1}{\omega-H_0}+
  \frac{1}{\omega-H_0}V\frac{1}{\omega-H}.
\end{equation}

To proceed further one needs to introduce the generalized matrix
elements:
\begin{equation}
  G_{nm}=\bra{0}\frac{a^n}{\sqrt{n!}}d\frac{1}{\omega-H}
  d^\dag\frac{(a^\dag)^m}{\sqrt{m!}}\ket{0},
\end{equation}
now introduce a set of zero electron $p$-phonon states and a set of
one electron $p$-phonon states
\begin{equation}
  \ket{0,p}=\frac{(a^\dag)^p}{\sqrt{p!}}\ket{0},
  \quad\ket{1,p}=\frac{(a^\dag)^p}{\sqrt{p!}}d^\dag\ket{0},
\end{equation}
one can write
\begin{equation}
  \begin{array}{rcl}
    G_{nm}&=&\displaystyle\bra{0}\frac{a^n}{\sqrt{n!}}d
    \frac{1}{\omega-H_0}d^\dag\frac{(a^\dag)^m}{\sqrt{m!}}\ket{0}+
    \bra{0}\frac{a^n}{\sqrt{n!}}d\frac{1}{\omega-H_0}V
    \frac{1}{\omega-H}d^\dag\frac{(a^\dag)^m}{\sqrt{m!}}\ket{0}\\\vbox to 20pt{}
    &=&\displaystyle G^{(0)}_{nm}+
    g\sum_{p_1,p_2}\bra{0}\frac{a^n}{\sqrt{n!}}d\frac{1}{\omega-H_0}d^\dag
    \ket{0,p_1}\bra{0,p_1}d(a+a^\dag)\ket{1,p_2}\bra{0,p_2}d
    \frac{1}{\omega-H}d^\dag\frac{(a^\dag)^m}{\sqrt{m!}}\ket{0}\\\vbox to 20pt{}
    &=&\displaystyle G^{(0)}_{nm}+
    g\sum_{p_1,p_2}G^{(0)}_{n,p_1}X_{p_1,p_2}G_{p_2,m}\\\vbox to 20pt{}
    &=&\displaystyle G^{(0)}_{nn}\delta_{nm}+
    g\sum_{p}G^{(0)}_{nn}X_{np}G_{pm}\quad,
  \end{array}
  \label{Polaron-Zero-Temperature-Element}
\end{equation}
where $G^{(0)}_{nn}(\omega)=G^{(0)}_{00}(\omega-n\omega_0)$ is the diagonal
element of the free Green's function, $X_{np}$ are the phonon
displacement matrix elements:
\begin{equation}
  X_{np}=\sqrt{p+1}\delta_{n,p+1}+\sqrt{p}\delta_{n,p-1}.
\end{equation}

Equation \eqref{Polaron-Zero-Temperature-Element} can be solved in
matrix notation:
\begin{equation}
  G^{-1}=G_0^{-1}-gX,
\end{equation}
it is easy to that $G^{-1}$ is a tridiagonal matrix.

Now define $T_k$ as the determinant of $G^{-1}$ with first $k$ rows
and columns removed, using Cramer's rule we find that
\begin{equation}
  G_{00}=\frac{T_1}{T_0},
\end{equation}
and define $D_k$ as the determinant comprising the first $k+1$ rows and
columns of $G^{-1}$ and $D_{-1}=1, D_{-2}=0$, then
\begin{equation}
  \begin{array}{l}
    D_0=[G^{(0)}]^{-1}_{00},\\
    D_1=[G^{(0)}]^{-1}_{11}[G^{(0)}]^{-1}_{00}-g^2
    =[G^{(0)}]^{-1}_{11}D_0-g^2,\\
    D_2=\cdots=[G^{(0)}]^{-1}_{2}D_1-2g^2D_0,\\
  \end{array}
\end{equation}
and, for the general case, the recurrence relations
\begin{equation}
  D_{k}=[G^{(0)}]^{-1}_{k,k}D_{k-1}-kg^2D_{k-2}.
\end{equation}
What's more, we have that
\begin{equation}
  T_k=[G^{(0)}]^{-1}_{kk}T_{k+1}-(k+1)g^2T_{k+2},\quad{\rm or}\quad
  \frac{T_k}{T_{k+1}}=[G^{(0)}]^{-1}_{kk}-(k+1)g^2\frac{T_{k+2}}{T_{k+1}},
\end{equation}
therefore
\begin{equation}
  \frac{T_1}{T_0}=\frac{1}{[G^{(0)}]^{-1}_{00}-g^2\frac{T_2}{T_1}}=\cdots,
\end{equation}
or
\begin{equation}
  G(\omega)=\frac{1}{\displaystyle G_0^{-1}(\omega)-
    \frac{g^2}{\displaystyle G_0^{-1}(\omega-\omega_0)-
      \frac{2g^2}{\displaystyle G_0^{-1}(\omega-2\omega_0)-
        \frac{3g^2}{\displaystyle G_0^{-1}(\omega-3\omega_0)-\cdots.}}}}
\end{equation}
Now use Dyson equation $\Sigma=G_0^{-1}-G^{-1}$ and we shall get
\begin{equation}
  \Sigma(\omega)=\frac{g^2}{\displaystyle G_0^{-1}(\omega-\omega_0)-
      \frac{2g^2}{\displaystyle G_0^{-1}(\omega-2\omega_0)-
        \frac{3g^2}{\displaystyle G_0^{-1}(\omega-3\omega_0)-\cdots.}}}
\end{equation}
The self-energy can be defined recursively,
\begin{equation}
  \Sigma^{(p)}(\omega)=\frac{pg^2}{G_0^{-1}(\omega-p\omega_0)-
    \Sigma^{(p+1)}}\quad.
\end{equation}

%%-----------------------------------------------------------------------
\subsection{The Finite Temperature Formalism}
At finite temperature, the trace performed over free phonon states gives
\begin{equation}
  G(\omega)=(1-e^{\beta\omega_0})\sum_ne^{-\beta n\omega_0}G_{nn}(\omega).
\end{equation}
Now we need to calculate $G_{nn}(\omega)$, according to $G^{-1}G=I$ we
have such a recurrence relation (recall that $G^{-1}$ is a tridiagonal
matrix):
\begin{equation}
  G_{nn}=G^{(0)}_n+gG^{(0)}_n(\sqrt{n}G_{n-1,n}+\sqrt{n+1}G_{n+1,n}),
\end{equation}
which we seek to write in a form as
\begin{equation}
    G_{nn}=G^{(0)}_n+G^{(0)}_n(AG_{nn}+BG_{nn}).
\end{equation}

Again according to Cramer's rule, 
\begin{equation}
  G_{n-1,n}=\sqrt{n}g\frac{D_{n-2}T_{n+1}}{T_0},\quad
  G_{nn}=\frac{D_{n-1}T_{n+1}}{T_0},
\end{equation}
recall the recurrence relation for $D$:
\begin{equation}
  D_k=[G^{(0)}_k]^{-1}D_{k-1}-kg^2D_{k-2},
\end{equation}
or
\begin{equation}
  \frac{D_{k-1}}{D_k}=\frac{1}{\displaystyle[G^{(0)}_k]^{-1}-kg^2
    \frac{D_{k-2}}{D_{k-1}}}\quad.
\end{equation}
Therefore 
\begin{equation}
  G_{n-1,n}=\sqrt{n}g\frac{D_{n-2}}{D_{n-1}}\frac{D_{n-1}T_{n+1}}{T_0}=
  \sqrt{n}g\frac{D_{n-2}}{D_{n-1}}G_{nn},
\end{equation}
i.e.,
\begin{equation}
  A=ng^2\frac{D_{n-2}}{D_{n-1}}=
  \frac{ng^2}{\displaystyle
    [G^{(0)}_n(\omega+\omega_0)]^{-1}-\frac{(n-1)g^2}{\displaystyle
      [G^{(0)}_n(\omega+2\omega_0)]^{-1}-\frac{(n-2)g^2}{\displaystyle
        \ddots-\frac{g^2}{[G^{(0)}_n(\omega+n\omega_0)]^{-1}}}}}
\end{equation}

Similarly, 
\begin{equation}
  G_{n+1,n}=\sqrt{n+1}g\frac{D_{n-1}T_{n+2}}{T_0}
  =\sqrt{n+1}g\frac{T_{n+2}}{T_{n+1}}G_{nn},
\end{equation}
recall the recurrence relation for $T$:
\begin{equation}
  T_k=[G^{(0)}_k]^{-1}T_{k+1}-(k+1)g^2T_{k+2},
\end{equation}
or
\begin{equation}
  \frac{T_{k+1}}{T_k}=\frac{1}{\displaystyle
    [G^{(0)}_k]^{-1}-(k+1)g^2\frac{T_{k+2}}{T_{k+1}}}.
\end{equation}
Therefore 
\begin{equation}
  B=(n+1)g^2\frac{T_{n+2}}{T_{n+1}}=
  \frac{(n+1)g^2}{\displaystyle
    [G^{(0)}_n(\omega-\omega_0)]^{-1}-\frac{(n+2)g^2}{\displaystyle
      [G^{(0)}_n(\omega-2\omega_0)]^{-1}-\frac{(n+3)g^2}{\displaystyle
        [G^{(0)}_n(\omega-3\omega_0)]^{-1}-\cdots,}}}
\end{equation}
finally 
\begin{equation}
  G_{nn}=\frac{1}{[G^{(0)}_n]^{-1}-A-B}.
\end{equation}
%%------------------------------------------------------------------------

\subsection{Dynamical Mean Field}
If we want to apply dynamical mean field theory, then a self
consistent condition is needed. Basically it is (see the solution for
simple impurity model)
\begin{equation}
  G^{-1}(\omega)=\omega-\sum_{ij}t_{oi}t_{jo}G^{(o)}_{ij}(\omega),
  \label{DMFT-self-consistent}
\end{equation}
where $G^{(o)}_{ij}$ is the Green's function with one site removed.
For Bethe lattice, it is very simple, in this case it is restricted
$i=j$, and in limit of infinite connectivity
$G^{(o)}_{ii}=G_{ii}$. Therefore for Bethe lattice
\begin{equation}
  G^{-1}(\omega)=\omega-t^2G(\omega).
\end{equation}
For a general lattice, the relation between the cavity and full
Green's functions reads
\begin{equation}
  G^{(o)}_{ij}=G_{ij}-\frac{G_{io}G_{oj}}{G_{oo}}.
\end{equation}

Therefore equation \eqref{DMFT-self-consistent} become
\begin{equation}
  G^{-1}=\omega-\sum_{ij}t_{oi}t_{jo}G_{ij}+
  \frac{\left(\sum_iG_{oi}\right)^2}{G_{oo}},
\end{equation}
recall that 
\begin{equation}
  G(\omega,k)=\frac{1}{\omega-\varepsilon_k-\Sigma(\omega)},
\end{equation}
we have that
\begin{equation}
  G^{-1}=\omega-\int d\varepsilon
  \frac{\rho(\varepsilon)\varepsilon^2}{\zeta-\varepsilon}
  -\left.\left(\int d\varepsilon
  \frac{\rho(\varepsilon)\varepsilon}{\zeta-\varepsilon}\right)^2\right/
  \int d\varepsilon\frac{\rho(\varepsilon)}{\zeta-\varepsilon},
\end{equation}
where $\zeta=\omega-\Sigma(\omega)$. This can be simplified further
using the following relations:
\begin{equation}
  \int d\varepsilon\frac{\rho(\varepsilon)\varepsilon^2}{\zeta-\varepsilon}
  =\zeta\int d\varepsilon
  \frac{\rho(\varepsilon)\varepsilon}{\zeta-\varepsilon},\quad
  \int d\varepsilon\frac{\rho(\varepsilon)}{\zeta-\varepsilon}=
  -1+\zeta\int d\varepsilon\frac{\rho(\varepsilon)}{\zeta-\varepsilon}.
\end{equation}
We have used $t_{oo}=\sum_kt_k=\int\rho(\varepsilon)\varepsilon=0$, finally
\begin{equation}
  G_0^{-1}=\Sigma+G^{-1}.
\end{equation}
\chapter{Physical Constants}
\begin{itemize}
\item The speed of light in vaccum, $c=299,792,458\;{\rm
  m/s}\approx3\times10^8\;{\rm m/s}$.
\item Electric charge $e=-1.602\times10^{-19}\;{\rm C}$.
\item energy in SI unit, joule $\rm J=kg\cdot(m/s)^2=N\cdot m=C\cdot V$.
\item Plank constant $h=6.62607004\times10^{-34}\;\rm{J\cdot s}=
  4.135667662\times10^{-15}\;{\rm eV\cdot s}$.
\item reduced Plank constant $\hbar=1.0545718\times10^{-34}\;{\rm J\cdot s}
  =6.582119514\times10^{-16}\;{\rm eV\cdot s}$
\item Boltzmann constant $k_B=1.38064852\times10^{-23}\;{\rm J\cdot K^{-1}}
  =8.6173324\times10^{-5}\;{\rm eV\cdot K^{-1}}$.
\item Bohr magneton $\mu_B=9.27400968\times10^{-24}\;{\rm J\cdot T^{-1}}
  =5.7883818066\times10^{-5}\;{\rm eV\cdot T^{-1}}$.
\item Bohr radius $a_0=5.29\times10^{-11}\;\rm m$.
\item Electron mass $m_e=9.10938215\times10^{-31}\;{\rm kg}
  =8.18710438\times10^{-14}\;{\rm J/c^2}=0.51099891\;{\rm MeV/c^2}$
\item Ohm $\Omega\rm=\frac{V}{A}=\frac{V\cdot s}{C}=\frac{J\cdot s}{C^2}
  =\frac{J}{s\cdot A^2}$.
\end{itemize}
\end{document}

